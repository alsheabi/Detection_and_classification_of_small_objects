# -*- coding: utf-8 -*-
"""Detection_and_classification_of_small_objects.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-iGXFB5HqrGG_5dBPtXkOAZebiEnzkf

#  Detection and classification of small objects

### This experiment works on both drones and chess datasets ([datasets](https://drive.google.com/drive/folders/1tyaq0c_YesoNot4c8n9M4uh89Njz7Sv_?usp=sharing))


Our code from the pipeline Repo [Ref](https://github.com/alsheabi/Yet-Another-EfficientDet-Pytorch)

# Setting up our envionment
"""

# Check GPU connection
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else: 
  print(gpu_info)

# Check GPU amount
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

# To how many GPU you used
if torch.cuda.is_available():
  print(torch.cuda.device_count())

import os
import random

#fixed version of tqdm output for Colab (tqdm is Py library for progress) 
!pip install --force https://github.com/chengs/tqdm/archive/colab.zip
#IGNORE restart runtime warning, it is indeed installed
#missing a few extra packages that we will need later! 
!pip install efficientnet_pytorch
!pip install tensorboardX
!pip install dicttoxml
!pip install  xmltodict
!pip install  torchvision==0.5
!pip install torch==1.4

"""# Mount drive and import dataset"""

# To Export drone dataset from drive, we mount drive to collabe.
from google.colab import drive
drive.mount("/content/drive")

# statistics for Chess dataset 
# train images
print("Count train Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Chess_Datasets/train') if name.endswith('.jpg')]))
# valid images
print("Count valid Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Chess_Datasets/valid') if name.endswith('.jpg')]))
# valid annotation files
print("Count valid Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Chess_Datasets/valid/Voc_Format') if name.endswith('.txt')]))
# test images
print("Count test Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Chess_Datasets/test') if name.endswith('.jpg')]))
# test annotation files
print("Count test Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Chess_Datasets/test/Voc_Format') if name.endswith('.txt')]))

# statistics for Full_UAV dataset 
# train images
print("Count train Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Full_UAV/train') if name.endswith('.jpg')]))
# valid images
print("Count valid Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Full_UAV/valid') if name.endswith('.jpg')]))
# valid annotation files
print("Count valid Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Full_UAV/valid/Voc_Format') if name.endswith('.txt')]))
# test images
print("Count test Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Full_UAV/test') if name.endswith('.jpg')]))
# test annotation files
print("Count test Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Full_UAV/test/Voc_Format') if name.endswith('.txt')]))

# statistics for MAV_VID dataset 
# train images
print("Count train Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/train') if name.endswith('.jpg')]))
# valid images
print("Count valid Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/valid') if name.endswith('.jpg')]))
# valid annotation files
print("Count valid Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/valid/Voc_Format') if name.endswith('.txt')]))
# test images
print("Count test Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/test') if name.endswith('.jpg')]))
# test annotation files
print("Count test Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/test/Voc_Format') if name.endswith('.txt')]))

# statistics for Combine (Full_UAV_AUAVD + MAV_VID) dataset 
# train images
print("Count train Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Combine_Drone/train') if name.endswith('.jpg')]))
# valid images
print("Count valid Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Combine_Drone/valid') if name.endswith('.jpg')]))
# valid annotation files
print("Count valid Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Combine_Drone/valid/Voc_Format') if name.endswith('.txt')]))
# test images
print("Count test Images: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Combine_Drone/test') if name.endswith('.jpg')]))
# test annotation files
print("Count test Ann: ",len([name for name in os.listdir('/content/drive/MyDrive/Dataset/Combine_Drone/test/Voc_Format') if name.endswith('.txt')]))

# How Images Look like 
imagepaths=[]
for i in range(5):
  test_images = [f for f in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/train') if f.endswith('.jpg')]
  #print(test_images)
  img_path = "/content/drive/MyDrive/Dataset/MAV_VID/train/" + random.choice(test_images);
  print(img_path)
  imagepaths.append(img_path)

# Display images
from IPython.display import Image
for path in imagepaths:
  x = Image(filename=path)
  display(x)

"""# Pre-processing training
## Below Cells must be execute before start training and evalaution step
In this section we set up the efficientDet-d0 model from backbone and dataloader with augmetation techniques to train our custom datasets
"""

#from train_detector import Detector
import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from tensorboardX import SummaryWriter
import shutil
import numpy as np
from tqdm.autonotebook import tqdm
import time
import random
class Detector():
    '''
    Class to train a detector
    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    '''
    def __init__(self, verbose=1):
        self.system_dict = {};
        self.system_dict["verbose"] = verbose;
        self.system_dict["local"] = {};
        self.system_dict["dataset"] = {};
        self.system_dict["dataset"]["train"] = {};
        self.system_dict["dataset"]["val"] = {};
        self.system_dict["dataset"]["val"]["status"] = False;
        self.system_dict["params"] = {};
        self.system_dict["params"]["image_size"] = 1024;
        self.system_dict["params"]["batch_size"] = 8;
        self.system_dict["params"]["num_workers"] = 2;#'--num_workers', type=int, default=12, help='num_workers of dataloader'
        self.system_dict["params"]["use_gpu"] = True;
        self.system_dict["params"]["gpu_devices"] = [0];
        self.system_dict["params"]["lr"] = 0.0001;
        self.system_dict["params"]["num_epochs"] = 50;
        self.system_dict["params"]["val_interval"] = 1;
        self.system_dict["params"]["es_min_delta"] = 0.0;
        self.system_dict["params"]["es_patience"] = 0;
        self.system_dict["output"] = {};
        self.system_dict["output"]["log_path"] = "tensorboard/signatrix_efficientdet_coco";
        self.system_dict["output"]["saved_path"] = "trained/";
        self.system_dict["output"]["best_epoch"] = 0;
        self.system_dict["output"]["best_loss"] = 1e5; #so 1e5 is equal to 100000



    def Train_Dataset(self, root_dir, coco_dir, img_dir, set_dir, batch_size=8, image_size=1024, use_gpu=True, num_workers=2):
        '''
        User function: Set training dataset parameters
        Dataset Directory Structure
                   root_dir
                      |
                      |------coco_dir 
                      |         |
                      |         |----img_dir
                      |                |
                      |                |------<set_dir_train> (set_dir) (Train)
                      |                         |
                      |                         |---------img1.jpg
                      |                         |---------img2.jpg
                      |                         |---------..........(and so on)  
                      |
                      |
                      |         |---annotations 
                      |         |----|
                      |              |--------------------instances_Train.json  (instances_<set_dir_train>.json)
                      |              |--------------------classes.txt
                      
                      
             - instances_Train.json -> In proper COCO format
             - classes.txt          -> A list of classes in alphabetical order
             
            For TrainSet
             - root_dir = "../sample_dataset";
             - coco_dir = "kangaroo";
             - img_dir = "images";
             - set_dir = "Train";
            
             
            Note: Annotation file name too coincides against the set_dir
        Args:
            root_dir (str): Path to root directory containing coco_dir
            coco_dir (str): Name of coco_dir containing image folder and annotation folder
            img_dir (str): Name of folder containing all training and validation folders
            set_dir (str): Name of folder containing all training images
            batch_size (int): Mini batch sampling size for training epochs
            image_size (int): Either of [512, 300]
            use_gpu (bool): If True use GPU else run on CPU
            num_workers (int): Number of parallel processors for data loader 
        Returns:
            None
        '''
        self.system_dict["dataset"]["train"]["root_dir"] = root_dir;
        self.system_dict["dataset"]["train"]["coco_dir"] = coco_dir;
        self.system_dict["dataset"]["train"]["img_dir"] = img_dir;
        self.system_dict["dataset"]["train"]["set_dir"] = set_dir;


        self.system_dict["params"]["batch_size"] = batch_size;
        self.system_dict["params"]["image_size"] = image_size;
        self.system_dict["params"]["use_gpu"] = use_gpu;
        self.system_dict["params"]["num_workers"] = num_workers;

        if(self.system_dict["params"]["use_gpu"]):
            if torch.cuda.is_available():
                self.system_dict["local"]["num_gpus"] = torch.cuda.device_count()
                torch.cuda.manual_seed(123)
            else:
                torch.manual_seed(123)

        self.system_dict["local"]["training_params"] = {"batch_size": self.system_dict["params"]["batch_size"] * self.system_dict["local"]["num_gpus"],
                                                           "shuffle": True,
                                                           "drop_last": True,
                                                           "collate_fn": collater,
                                                           "num_workers": self.system_dict["params"]["num_workers"]}

        self.system_dict["local"]["training_set"] = CocoDataset(root_dir=self.system_dict["dataset"]["train"]["root_dir"]+"/" + self.system_dict["dataset"]["train"]["coco_dir"],
                                                            img_dir = self.system_dict["dataset"]["train"]["img_dir"],
                                                            set_dir = self.system_dict["dataset"]["train"]["set_dir"],
                                                            # Here add Augmenter to transform compose(Augmenter_flip_h,Augmenter_flip_v,Augmenter_grayscale,Augmenter_hue,Augmenter_saturation,Augmenter_value,Augmenter_sv,Augment_hsv,Augmenter_s_or_v,Augmenter_RandomFlip,Augmenter_FlipHV)
                                                            # such as :
                                                            # transform = transforms.Compose([Augmenter_flip_h(),Normalizer(),Resizer(common_size = self.system_dict["params"]["image_size"])])
                                                            # transform = transforms.Compose([Augmenter_grayscale(),Normalizer(),Resizer(common_size = self.system_dict["params"]["image_size"])])
                                                            transform = transforms.Compose([Normalizer(),Resizer(common_size = self.system_dict["params"]["image_size"])])
                                                            )  

        self.system_dict["local"]["training_generator"] = DataLoader(self.system_dict["local"]["training_set"], 
                                                                    **self.system_dict["local"]["training_params"]);


    def Val_Dataset(self, root_dir, coco_dir, img_dir, set_dir):
        '''
        User function: Set training dataset parameters
        Dataset Directory Structure
                   root_dir
                      |
                      |------coco_dir 
                      |         |
                      |         |----img_dir
                      |                |
                      |                |------<set_dir_val> (set_dir) (Validation)
                      |                         |
                      |                         |---------img1.jpg
                      |                         |---------img2.jpg
                      |                         |---------..........(and so on)  
                      |
                      |
                      |         |---annotations 
                      |         |----|
                      |              |--------------------instances_Val.json  (instances_<set_dir_val>.json)
                      |              |--------------------classes.txt
                      
                      
             - instances_Train.json -> In proper COCO format
             - classes.txt          -> A list of classes in alphabetical order
             
            For ValSet
             - root_dir = "..sample_dataset";
             - coco_dir = "kangaroo";
             - img_dir = "images";
             - set_dir = "Val";
             
             Note: Annotation file name too coincides against the set_dir
        Args:
            root_dir (str): Path to root directory containing coco_dir
            coco_dir (str): Name of coco_dir containing image folder and annotation folder
            img_dir (str): Name of folder containing all training and validation folders
            set_dir (str): Name of folder containing all validation images
        Returns:
            None
        '''
        self.system_dict["dataset"]["val"]["status"] = True;
        self.system_dict["dataset"]["val"]["root_dir"] = root_dir;
        self.system_dict["dataset"]["val"]["coco_dir"] = coco_dir;
        self.system_dict["dataset"]["val"]["img_dir"] = img_dir;
        self.system_dict["dataset"]["val"]["set_dir"] = set_dir;     

        self.system_dict["local"]["val_params"] = {"batch_size": self.system_dict["params"]["batch_size"],
                                                   "shuffle": False,
                                                   "drop_last": False,
                                                   "collate_fn": collater,
                                                   "num_workers": self.system_dict["params"]["num_workers"]}

        self.system_dict["local"]["val_set"] = CocoDataset(root_dir=self.system_dict["dataset"]["val"]["root_dir"] + "/" + self.system_dict["dataset"]["val"]["coco_dir"], 
                                                    img_dir = self.system_dict["dataset"]["val"]["img_dir"],
                                                    set_dir = self.system_dict["dataset"]["val"]["set_dir"],
                                                    transform=transforms.Compose([Normalizer(), Resizer(common_size = self.system_dict["params"]["image_size"])]))
        
        self.system_dict["local"]["test_generator"] = DataLoader(self.system_dict["local"]["val_set"], 
                                                                **self.system_dict["local"]["val_params"])

    
    #efficientnet-b0;
    def Model(self, model_name="efficientnet-b0", gpu_devices=[0], load_pretrained_model_from=None):
        '''
        User function: Set Model parameters
        Args:
            gpu_devices (list): List of GPU Device IDs to be used in training
        Returns:
            None
        '''
        if(not load_pretrained_model_from):
            num_classes = self.system_dict["local"]["training_set"].num_classes();
            print("Number of classes = ",num_classes)
            coeff = int(model_name[-1]) 
            efficientdet = EfficientDet(num_classes=num_classes, compound_coef=coeff, model_name=model_name);

            if self.system_dict["params"]["use_gpu"]:
                self.system_dict["params"]["gpu_devices"] = gpu_devices
                if len(self.system_dict["params"]["gpu_devices"])==1:
                    os.environ["CUDA_VISIBLE_DEVICES"] = str(self.system_dict["params"]["gpu_devices"][0])
                    print("gpu_devices is 1 :",str(self.system_dict["params"]["gpu_devices"][0]))
                else:
                    os.environ["CUDA_VISIBLE_DEVICES"] = ','.join([str(id) for id in self.system_dict["params"]["gpu_devices"]])
                self.system_dict["local"]["device"] = 'cuda' if torch.cuda.is_available() else 'cpu'
                efficientdet = efficientdet.to(self.system_dict["local"]["device"])
                efficientdet= torch.nn.DataParallel(efficientdet).to(self.system_dict["local"]["device"])

            self.system_dict["local"]["model"] = efficientdet;
            self.system_dict["local"]["model"].train();
        else:
            efficientdet = torch.load(load_pretrained_model_from).module
            if self.system_dict["params"]["use_gpu"]:
                self.system_dict["params"]["gpu_devices"] = gpu_devices
                if len(self.system_dict["params"]["gpu_devices"])==1:
                    os.environ["CUDA_VISIBLE_DEVICES"] = str(self.system_dict["params"]["gpu_devices"][0])
                else:
                    os.environ["CUDA_VISIBLE_DEVICES"] = ','.join([str(id) for id in self.system_dict["params"]["gpu_devices"]])
                self.system_dict["local"]["device"] = 'cuda' if torch.cuda.is_available() else 'cpu'
                efficientdet = efficientdet.to(self.system_dict["local"]["device"])
                efficientdet= torch.nn.DataParallel(efficientdet).to(self.system_dict["local"]["device"])
            
            self.system_dict["local"]["model"] = efficientdet;
            self.system_dict["local"]["model"].train();
       


    def Set_Hyperparams(self, lr=0.0001, val_interval=1, es_min_delta=0.0, es_patience=0):
        '''
        User function: Set hyper parameters
        Args:
            lr (float): Initial learning rate for training
            val_interval (int): Post specified number of training epochs, a validation epoch will be carried out
            es_min_delta (float): Loss detla value, if loss doesnn't change more than this value for "es_patience" number of epochs, training will be stopped early
            es_patience (int): If loss doesnn't change more than this "es_min_delta" value for "es_patience" number of epochs, training will be stopped early
        Returns:
            None
        '''
        self.system_dict["params"]["lr"] = lr;
        self.system_dict["params"]["val_interval"] = val_interval;
        self.system_dict["params"]["es_min_delta"] = es_min_delta;
        self.system_dict["params"]["es_patience"] = es_patience; # see https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html


        self.system_dict["local"]["optimizer"] = torch.optim.Adam(self.system_dict["local"]["model"].parameters(), 
                                                                    self.system_dict["params"]["lr"]);
        # or use >> optimizer = torch.optim.SGD(self.system_dict["local"]["model"].parameters(), self.system_dict["params"]["lr"], momentum=0.9, nesterov=True)
        self.system_dict["local"]["scheduler"] = torch.optim.lr_scheduler.ReduceLROnPlateau(self.system_dict["local"]["optimizer"], 
                                                                    patience=3, verbose=True)


    def Train(self, num_epochs=2, model_output_dir="trained/",model_output_tensorboard='tensorboard/signatrix_efficientdet_coco/'):
        '''
        User function: Start training
        Args:
            num_epochs (int): Number of epochs to train for
            model_output_dir (str): Path to directory where all trained models will be saved
            model_output_tensorboard (str): Path to directory where all result show on tensorboard will be saved
        Returns:
            None
        '''
        # compute time excution
        start = time.time()
        # save tensorboard result in google drive
        self.system_dict["output"]["log_path"] = model_output_tensorboard;
        #self.system_dict["output"]["log_path"] = "tensorboard/signatrix_efficientdet_coco";
        self.system_dict["output"]["saved_path"] = model_output_dir;
        self.system_dict["params"]["num_epochs"] = num_epochs;

        if os.path.isdir(self.system_dict["output"]["log_path"]):
            shutil.rmtree(self.system_dict["output"]["log_path"])
        os.makedirs(self.system_dict["output"]["log_path"])

        if os.path.isdir(self.system_dict["output"]["saved_path"]):
            shutil.rmtree(self.system_dict["output"]["saved_path"])
        os.makedirs(self.system_dict["output"]["saved_path"])

        writer = SummaryWriter(self.system_dict["output"]["log_path"])

        num_iter_per_epoch = len(self.system_dict["local"]["training_generator"])
        print("num_iter_per_epoch :",num_iter_per_epoch)
        # with open(model_output_dir + "/train_output.txt", 'a') as output_file:
        #   output_file.write("\n# num_iter_per_epoch :" +str( num_iter_per_epoch )+ "\n")
        text ="\n# num_iter_per_epoch :" +str( num_iter_per_epoch )+ "\n"
                       
        if (self.system_dict["dataset"]["val"]["status"]):
            cls_loss = []
            reg_loss = []

            for epoch in range(self.system_dict["params"]["num_epochs"]):
                self.system_dict["local"]["model"].train()
                loss_regression_ls = []
                loss_classification_ls = []
                epoch_loss = []
                progress_bar = tqdm(self.system_dict["local"]["training_generator"])
                #print("progress_bar : ",progress_bar)
                total_loss=0
                for iter, data in enumerate(progress_bar):
                  try:
                      self.system_dict["local"]["optimizer"].zero_grad()
                      if torch.cuda.is_available():
                          cls_loss, reg_loss = self.system_dict["local"]["model"]([data['img'].to(self.system_dict["local"]["device"]).float(), data['annot'].to(self.system_dict["local"]["device"])])
                      else:
                          cls_loss, reg_loss = self.system_dict["local"]["model"]([data['img'].float(), data['annot']])

                      cls_loss = cls_loss.mean()
                      #print("cls_loss mean :", cls_loss)
                      reg_loss = reg_loss.mean()
                      #print("reg_loss mean :", reg_loss)
                      loss = cls_loss + reg_loss
                      #print("cls_loss + reg_loss: ",loss)
                      if loss == 0:
                          print("loss is zero !!! continue....")
                          continue
                      loss_classification_ls.append(float(cls_loss))
                      loss_regression_ls.append(float(reg_loss))
                      loss.backward()
                      # comment below line ?
                      torch.nn.utils.clip_grad_norm_(self.system_dict["local"]["model"].parameters(), 0.1)
                      self.system_dict["local"]["optimizer"].step()
                      epoch_loss.append(float(loss))
                      total_loss= np.mean(epoch_loss)
                      progress_bar.set_description(
                            'Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Batch loss: {:.5f} Total loss: {:.5f}'.format(
                                epoch, self.system_dict["params"]["num_epochs"], iter + 1, num_iter_per_epoch, cls_loss, reg_loss, loss,
                                total_loss))#loss

                  except Exception as e:
                      print(e)
                      continue
                cls_loss = np.mean(loss_classification_ls)
                reg_loss = np.mean(loss_regression_ls)
                
                train_loss = cls_loss + reg_loss
                print(
                    'Epoch: {}/{}.  train Cls loss: {:.5f}. train Reg loss: {:.5f}.  train Total loss: {:.5f}'.format(
                        epoch  , self.system_dict["params"]["num_epochs"], cls_loss, reg_loss, np.mean(train_loss)))
                text +=('Epoch: {}/{}.  train Cls loss: {:.5f}. train Reg loss: {:.5f}.  train Total loss: {:.5f}'.format(
                        epoch  , self.system_dict["params"]["num_epochs"], cls_loss, reg_loss, np.mean(train_loss))+ "\n")
                writer.add_scalar('Train/Total_loss', train_loss, epoch)#epoch * num_iter_per_epoch + iter)
                writer.add_scalar('Train/Regression_loss', reg_loss, epoch )#epoch * num_iter_per_epoch + iter)
                writer.add_scalar('Train/Classfication_loss (focal loss)', cls_loss, epoch )#epoch * num_iter_per_epoch + iter)
                self.system_dict["local"]["scheduler"].step(np.mean(epoch_loss))



                # to validate our training
                if epoch % self.system_dict["params"]["val_interval"] == 0: # epoch % 1 ==0
                    print("do val !!!!!")
                    self.system_dict["local"]["model"].eval()
                    loss_regression_ls = []
                    loss_classification_ls = []
                    epoch_loss=[]
                    progress_bar_val = tqdm(self.system_dict["local"]["test_generator"])
                    for iter, data in enumerate(progress_bar_val):
                        with torch.no_grad():
                            if torch.cuda.is_available():
                                cls_loss, reg_loss = self.system_dict["local"]["model"]([data['img'].to(self.system_dict["local"]["device"]).float(), data['annot'].to(self.system_dict["local"]["device"])])
                            else:
                                cls_loss, reg_loss = self.system_dict["local"]["model"]([data['img'].float(), data['annot']])

                            cls_loss = cls_loss.mean()
                            reg_loss = reg_loss.mean()
                            loss=cls_loss+reg_loss
                            loss_classification_ls.append(float(cls_loss))
                            loss_regression_ls.append(float(reg_loss))
                            epoch_loss.append(float(loss))
                            total_loss= np.mean(epoch_loss)
                            progress_bar_val.set_description('Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Batch loss: {:.5f} Total loss: {:.5f}'.format(
                                epoch, self.system_dict["params"]["num_epochs"], iter + 1, num_iter_per_epoch, cls_loss, reg_loss, loss,
                                total_loss))
                    cls_loss = np.mean(loss_classification_ls)
                    reg_loss = np.mean(loss_regression_ls)
                    val_loss = cls_loss + reg_loss

                    print(
                        'Epoch: {}/{}. val Classification loss: {:1.5f}. val Regression loss: {:1.5f}. val Total loss: {:1.5f}'.format(
                            epoch , self.system_dict["params"]["num_epochs"], cls_loss, reg_loss,
                            np.mean(val_loss)))
                    text+=('Epoch: {}/{}. val Classification loss: {:1.5f}. val Regression loss: {:1.5f}. val Total loss: {:1.5f}'.format(
                            epoch , self.system_dict["params"]["num_epochs"], cls_loss, reg_loss,
                            np.mean(val_loss))+"\n")
                    text+=("Error value B/W Train loss and Val lss : "+str(val_loss - train_loss)+"\n")
                    print("Error value B/W Train loss and Val lss : ",val_loss - train_loss)
                    writer.add_scalar('Val/Total_loss', val_loss, epoch)
                    writer.add_scalar('Val/Regression_loss', reg_loss, epoch)
                    writer.add_scalar('Val/Classfication_loss (focal loss)', cls_loss, epoch)

                    if val_loss + self.system_dict["params"]["es_min_delta"] < self.system_dict["output"]["best_loss"]:
                        self.system_dict["output"]["best_loss"] = val_loss
                        best_train_loss=train_loss
                        self.system_dict["output"]["best_epoch"] = epoch
                        self.system_dict["params"]["es_patience"] = 0
                        print("restart counter :  ",self.system_dict["params"]["es_patience"])
                        text+=("restart counter :  "+str(self.system_dict["params"]["es_patience"])+"\n")
                        torch.save(self.system_dict["local"]["model"], 
                            os.path.join(self.system_dict["output"]["saved_path"], "signatrix_efficientdet_coco.pth"))
                        end = time.time()
                        hours, rem = divmod(end-start, 3600)
                        minutes, seconds = divmod(rem, 60)
                        print("Time Execution  in best loss : {:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds))
                        text+=("Time Execution  in best loss : {:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds)+"\n")
                        dummy_input = torch.rand(1, 3, 1024, 1024)
                        if torch.cuda.is_available():
                            dummy_input = dummy_input.cuda()
                        if isinstance(self.system_dict["local"]["model"], nn.DataParallel):
                            self.system_dict["local"]["model"].module.backbone_net.model.set_swish(memory_efficient=False)

                            torch.onnx.export(self.system_dict["local"]["model"].module, dummy_input,
                                              os.path.join(self.system_dict["output"]["saved_path"], "signatrix_efficientdet_coco.onnx"),
                                              verbose=False,opset_version=11)
                            self.system_dict["local"]["model"].module.backbone_net.model.set_swish(memory_efficient=True)
                        else:
                            self.system_dict["local"]["model"].backbone_net.model.set_swish(memory_efficient=False)

                            torch.onnx.export(self.system_dict["local"]["model"], dummy_input,
                                              os.path.join(self.system_dict["output"]["saved_path"], "signatrix_efficientdet_coco.onnx"),
                                              verbose=False,opset_version=11)
                            self.system_dict["local"]["model"].backbone_net.model.set_swish(memory_efficient=True)
                    else:
                          self.system_dict["params"]["es_patience"] = self.system_dict["params"]["es_patience"] + 1
                          print("Increment counter: ",self.system_dict["params"]["es_patience"])
                          text+=("Increment counter: "+str(self.system_dict["params"]["es_patience"])+"\n")

                    # Early stopping 
                    #epoch - self.system_dict["output"]["best_epoch"] >
                    if  self.system_dict["params"]["es_patience"] > 5: # stop early when no more improve on performance patience 
                        print("Stop training at epoch {}. The last loss achieved is {}. The best loss is {}. At best Epochs {}. With es_patience {}."
                        .format(epoch, val_loss,self.system_dict["output"]["best_loss"],self.system_dict["output"]["best_epoch"],
                                self.system_dict["params"]["es_patience"]  ))
                        text+=("Stop training at epoch {}. The last loss achieved is {}. The best loss is {}. At best Epochs {}. With es_patience {}."
                        .format(epoch, val_loss,self.system_dict["output"]["best_loss"],self.system_dict["output"]["best_epoch"],
                                self.system_dict["params"]["es_patience"] )+"\n")
                        print("Error value B/W Train loss and Val lss : ",self.system_dict["output"]["best_loss"] - best_train_loss)
                        text+=("Error value B/W Train loss and Val lss : "+str(self.system_dict["output"]["best_loss"] - best_train_loss)+"\n")

                        # output_file.write(text)
                        #output_file.close()
                        break
            end = time.time()
            hours, rem = divmod(end-start, 3600)
            minutes, seconds = divmod(rem, 60)
            print("Time Execution : {:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds))
            text+=("{:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds)+"\n")
            with open(model_output_dir + "/train_output.txt", 'a') as output_file:
              output_file.write(text)
            # output_file.write(text)
        else: 
            # only trainset and no Val
            for epoch in range(self.system_dict["params"]["num_epochs"]):
                self.system_dict["local"]["model"].train()

                epoch_loss = []
                progress_bar = tqdm(self.system_dict["local"]["training_generator"])
                #print("progress_bar : ",progress_bar)
                for iter, data in enumerate(progress_bar):
                    try:
                        self.system_dict["local"]["optimizer"].zero_grad()
                        if torch.cuda.is_available():
                            cls_loss, reg_loss = self.system_dict["local"]["model"]([data['img'].to(self.system_dict["local"]["device"]).float(), data['annot'].to(self.system_dict["local"]["device"])])
                        else:
                            cls_loss, reg_loss = self.system_dict["local"]["model"]([data['img'].float(), data['annot']])

                        cls_loss = cls_loss.mean()
                        reg_loss = reg_loss.mean()
                        loss = cls_loss + reg_loss
                        if loss == 0:
                            continue # if loss equal 0 will skip and go for next images
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.system_dict["local"]["model"].parameters(), 0.1)
                        self.system_dict["local"]["optimizer"].step()
                        epoch_loss.append(float(loss))
                        total_loss = np.mean(epoch_loss)

                        progress_bar.set_description(
                            'Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Batch loss: {:.5f} Total loss: {:.5f}'.format(
                                epoch , self.system_dict["params"]["num_epochs"], iter + 1, num_iter_per_epoch, cls_loss, reg_loss, loss,
                                total_loss))
                        writer.add_scalar('Train/Total_loss', total_loss, epoch * num_iter_per_epoch + iter)
                        writer.add_scalar('Train/Regression_loss', reg_loss, epoch * num_iter_per_epoch + iter)
                        writer.add_scalar('Train/Classfication_loss (focal loss)', cls_loss, epoch * num_iter_per_epoch + iter)

                    except Exception as e:
                        print(e)
                        continue
                self.system_dict["local"]["scheduler"].step(np.mean(epoch_loss))


                torch.save(self.system_dict["local"]["model"], 
                    os.path.join(self.system_dict["output"]["saved_path"], "signatrix_efficientdet_coco.pth"))

                dummy_input = torch.rand(1, 3, 1024, 1024)
                if torch.cuda.is_available():
                    dummy_input = dummy_input.to(self.system_dict["local"]["device"])
                if isinstance(self.system_dict["local"]["model"], nn.DataParallel):
                    self.system_dict["local"]["model"].module.backbone_net.model.set_swish(memory_efficient=False)

                    torch.onnx.export(self.system_dict["local"]["model"].module, dummy_input,
                                      os.path.join(self.system_dict["output"]["saved_path"], "signatrix_efficientdet_coco.onnx"),
                                      verbose=False,opset_version=11)
                    self.system_dict["local"]["model"].module.backbone_net.model.set_swish(memory_efficient=True)
                else:
                    self.system_dict["local"]["model"].backbone_net.model.set_swish(memory_efficient=False)

                    torch.onnx.export(self.system_dict["local"]["model"], dummy_input,
                                      os.path.join(self.system_dict["output"]["saved_path"], "signatrix_efficientdet_coco.onnx"),
                                      verbose=False,opset_version=11)
                    self.system_dict["local"]["model"].backbone_net.model.set_swish(memory_efficient=True)

        writer.close()

import torch
import torch.nn as nn
import numpy as np
class BBoxTransform(nn.Module):

    def __init__(self, mean=None, std=None):
        super(BBoxTransform, self).__init__()
        if mean is None:
            self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32))
        else:
            self.mean = mean
        if std is None:
            self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))
        else:
            self.std = std
        if torch.cuda.is_available():
            self.mean = self.mean.cuda()
            self.std = self.std.cuda()

    def forward(self, boxes, deltas):

        widths = boxes[:, :, 2] - boxes[:, :, 0]
        heights = boxes[:, :, 3] - boxes[:, :, 1]
        ctr_x = boxes[:, :, 0] + 0.5 * widths
        ctr_y = boxes[:, :, 1] + 0.5 * heights

        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]
        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]
        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]
        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]

        pred_ctr_x = ctr_x + dx * widths
        pred_ctr_y = ctr_y + dy * heights
        pred_w = torch.exp(dw) * widths
        pred_h = torch.exp(dh) * heights

        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w
        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h
        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w
        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h

        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)

        return pred_boxes


class ClipBoxes(nn.Module):

    def __init__(self):
        super(ClipBoxes, self).__init__()

    def forward(self, boxes, img):
        batch_size, num_channels, height, width = img.shape

        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)
        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)

        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)
        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)

        return boxes

    """
    adapted and modified from https://github.com/google/automl/blob/master/efficientdet/anchors.py by Zylo117
    """
class Anchors(nn.Module):
    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):
        super(Anchors, self).__init__()

        if pyramid_levels is None:
            self.pyramid_levels = [3, 4, 5, 6, 7]
        if strides is None:
            self.strides = [2 ** x for x in self.pyramid_levels]
        if sizes is None:
            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]#2^(3+2)=32
        if ratios is None:
            #self.ratios = np.array([(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]) not works
            #self.ratios = np.array([0.834, 1, 1.3]) #worse in Full
            #self.ratios = np.array([0.434, 1, 1]) worse in Full
            self.ratios = np.array([0.634, 1, 1.577]) #good in Full
            #self.ratios = np.array([0.5, 1, 2]) not bad in Full
            #self.ratios = np.array([1, 1, 2]) 

        if scales is None:
            #self.scales = np.array([2, 1.2599210498948732, 1.5874010519681994])
            #self.scales = np.array([1, 0.506, 0.641])
            self.scales = np.array([0.4, 0.506, 0.641])
            #self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])#[1, 1.2599210498948732, 1.5874010519681994]

    def forward(self, image):
        """Generates multiscale anchor boxes.
        Args:
          image_size: integer number of input image size. The input image has the
            same dimension for width and height. The image_size should be divided by
            the largest feature stride 2^max_level.
          anchor_scale: float number representing the scale of size of the base
            anchor to the feature stride 2^level.
          anchor_configs: a dictionary with keys as the levels of anchors and
            values as a list of anchor configuration.
        Returns:
          anchor_boxes: a numpy array with shape [N, 4], which stacks anchors on all
            feature levels.
        Raises:
          ValueError: input size must be the multiple of largest feature stride.
        """
        image_shape = image.shape[2:]
        image_shape = np.array(image_shape)
        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]

        all_anchors = np.zeros((0, 4)).astype(np.float32)

        for idx, p in enumerate(self.pyramid_levels):
            anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)
            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)
            all_anchors = np.append(all_anchors, shifted_anchors, axis=0)

        all_anchors = np.expand_dims(all_anchors, axis=0)

        anchors = torch.from_numpy(all_anchors.astype(np.float32))
        if torch.cuda.is_available():
            anchors = anchors.cuda()
        return anchors


def generate_anchors(base_size=16, ratios=None, scales=None):
    if ratios is None:
        ratios = np.array([0.634, 1, 1.577])
    if scales is None:
        scales = np.array([0.4, 0.506, 0.641])

    num_anchors = len(ratios) * len(scales)
    anchors = np.zeros((num_anchors, 4))
    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T
    areas = anchors[:, 2] * anchors[:, 3]
    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))
    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))
    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T
    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T

    return anchors


def compute_shape(image_shape, pyramid_levels):
    image_shape = np.array(image_shape[:2])
    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]
    return image_shapes


def shift(shape, stride, anchors):
    shift_x = (np.arange(0, shape[1]) + 0.5) * stride
    shift_y = (np.arange(0, shape[0]) + 0.5) * stride
    shift_x, shift_y = np.meshgrid(shift_x, shift_y)
    shifts = np.vstack((
        shift_x.ravel(), shift_y.ravel(),
        shift_x.ravel(), shift_y.ravel()
    )).transpose()

    A = anchors.shape[0]
    K = shifts.shape[0]
    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))
    all_anchors = all_anchors.reshape((K * A, 4))

    return all_anchors

import torch
import torch.nn as nn

def calc_iou(a, b):

    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])
    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])
    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])
    iw = torch.clamp(iw, min=0)
    ih = torch.clamp(ih, min=0)
    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih
    ua = torch.clamp(ua, min=1e-8)
    intersection = iw * ih
    IoU = intersection / ua

    return IoU


class FocalLoss(nn.Module):
    def __init__(self):
        super(FocalLoss, self).__init__()

    def forward(self, classifications, regressions, anchors, annotations):
        alpha = 0.25
        gamma = 2.0
        batch_size = classifications.shape[0]
        classification_losses = []
        regression_losses = []

        anchor = anchors[0, :, :]

        anchor_widths = anchor[:, 2] - anchor[:, 0]
        anchor_heights = anchor[:, 3] - anchor[:, 1]
        anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths
        anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights

        for j in range(batch_size):

            classification = classifications[j, :, :]
            regression = regressions[j, :, :]

            bbox_annotation = annotations[j, :, :]
            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]

            if bbox_annotation.shape[0] == 0:
                if torch.cuda.is_available():
                    regression_losses.append(torch.tensor(0).float().cuda())
                    classification_losses.append(torch.tensor(0).float().cuda())
                else:
                    regression_losses.append(torch.tensor(0).float())
                    classification_losses.append(torch.tensor(0).float())

                continue

            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)

            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4])

            IoU_max, IoU_argmax = torch.max(IoU, dim=1)

            # compute the loss for classification
            targets = torch.ones(classification.shape) * -1
            if torch.cuda.is_available():
                targets = targets.cuda()
            # this line means that targets with iou lower than 0.4 are considered as background
            targets[torch.lt(IoU_max, 0.4), :] = 0
            # this line records indices where iou is greater than 0.5 and thus correspond to objects.-
            positive_indices = torch.ge(IoU_max, 0.5)

            num_positive_anchors = positive_indices.sum()

            assigned_annotations = bbox_annotation[IoU_argmax, :]

            targets[positive_indices, :] = 0
            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1

            alpha_factor = torch.ones(targets.shape) * alpha
            if torch.cuda.is_available():
                alpha_factor = alpha_factor.cuda()

            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)
            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)
            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)

            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))

            cls_loss = focal_weight * bce

            zeros = torch.zeros(cls_loss.shape)
            if torch.cuda.is_available():
                zeros = zeros.cuda()
            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)

            classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.float(), min=1.0))


            if positive_indices.sum() > 0:
                assigned_annotations = assigned_annotations[positive_indices, :]

                anchor_widths_pi = anchor_widths[positive_indices]
                anchor_heights_pi = anchor_heights[positive_indices]
                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]
                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]

                gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0]
                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]
                gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths
                gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights

                gt_widths = torch.clamp(gt_widths, min=1)
                gt_heights = torch.clamp(gt_heights, min=1)

                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi
                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi
                targets_dw = torch.log(gt_widths / anchor_widths_pi)
                targets_dh = torch.log(gt_heights / anchor_heights_pi)

                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))
                targets = targets.t()

                norm = torch.Tensor([[0.1, 0.1, 0.2, 0.2]])
                if torch.cuda.is_available():
                    norm = norm.cuda()
                targets = targets / norm

                regression_diff = torch.abs(targets - regression[positive_indices, :])

                regression_loss = torch.where(
                    torch.le(regression_diff, 1.0 / 9.0),
                    0.5 * 9.0 * torch.pow(regression_diff, 2),
                    regression_diff - 0.5 / 9.0
                )
                regression_losses.append(regression_loss.mean())
            else:
                if torch.cuda.is_available():
                    regression_losses.append(torch.tensor(0).float().cuda())
                else:
                    regression_losses.append(torch.tensor(0).float())

        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0,
                                                                                                                 keepdim=True)

import torch.nn as nn
import torch
import math
from efficientnet_pytorch import EfficientNet as EffNet
#from src.utils import BBoxTransform, ClipBoxes, Anchors
#from src.loss import FocalLoss
from torchvision.ops.boxes import nms as nms_torch

def nms(dets, thresh):
    return nms_torch(dets[:, :4], dets[:, 4], thresh)

class ConvBlock(nn.Module):
    def __init__(self, num_channels):
        super(ConvBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1, groups=num_channels),
            nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(num_features=num_channels, momentum=0.9997, eps=4e-5), nn.ReLU())

    def forward(self, input):
        return self.conv(input)


class BiFPN(nn.Module):
    def __init__(self, num_channels, epsilon=1e-4):
        super(BiFPN, self).__init__()
        self.epsilon = epsilon
        # Conv layers
        self.conv6_up = ConvBlock(num_channels)
        self.conv5_up = ConvBlock(num_channels)
        self.conv4_up = ConvBlock(num_channels)
        self.conv3_up = ConvBlock(num_channels)
        self.conv4_down = ConvBlock(num_channels)
        self.conv5_down = ConvBlock(num_channels)
        self.conv6_down = ConvBlock(num_channels)
        self.conv7_down = ConvBlock(num_channels)

        # Feature scaling layers
        self.p6_upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.p5_upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.p4_upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.p3_upsample = nn.Upsample(scale_factor=2, mode='nearest')

        self.p4_downsample = nn.MaxPool2d(kernel_size=2)
        self.p5_downsample = nn.MaxPool2d(kernel_size=2)
        self.p6_downsample = nn.MaxPool2d(kernel_size=2)
        self.p7_downsample = nn.MaxPool2d(kernel_size=2)

        # Weight
        self.p6_w1 = nn.Parameter(torch.ones(2))
        self.p6_w1_relu = nn.ReLU()
        self.p5_w1 = nn.Parameter(torch.ones(2))
        self.p5_w1_relu = nn.ReLU()
        self.p4_w1 = nn.Parameter(torch.ones(2))
        self.p4_w1_relu = nn.ReLU()
        self.p3_w1 = nn.Parameter(torch.ones(2))
        self.p3_w1_relu = nn.ReLU()

        self.p4_w2 = nn.Parameter(torch.ones(3))
        self.p4_w2_relu = nn.ReLU()
        self.p5_w2 = nn.Parameter(torch.ones(3))
        self.p5_w2_relu = nn.ReLU()
        self.p6_w2 = nn.Parameter(torch.ones(3))
        self.p6_w2_relu = nn.ReLU()
        self.p7_w2 = nn.Parameter(torch.ones(2))
        self.p7_w2_relu = nn.ReLU()

    def forward(self, inputs):
        """
            P7_0 -------------------------- P7_2 -------->
            P6_0 ---------- P6_1 ---------- P6_2 -------->
            P5_0 ---------- P5_1 ---------- P5_2 -------->
            P4_0 ---------- P4_1 ---------- P4_2 -------->
            P3_0 -------------------------- P3_2 -------->
        """

        # P3_0, P4_0, P5_0, P6_0 and P7_0
        p3_in, p4_in, p5_in, p6_in, p7_in = inputs
        # P7_0 to P7_2
        # Weights for P6_0 and P7_0 to P6_1
        p6_w1 = self.p6_w1_relu(self.p6_w1)
        weight = p6_w1 / (torch.sum(p6_w1, dim=0) + self.epsilon)
        # Connections for P6_0 and P7_0 to P6_1 respectively
        p6_up = self.conv6_up(weight[0] * p6_in + weight[1] * self.p6_upsample(p7_in))
        # Weights for P5_0 and P6_0 to P5_1
        p5_w1 = self.p5_w1_relu(self.p5_w1)
        weight = p5_w1 / (torch.sum(p5_w1, dim=0) + self.epsilon)
        # Connections for P5_0 and P6_0 to P5_1 respectively
        p5_up = self.conv5_up(weight[0] * p5_in + weight[1] * self.p5_upsample(p6_up))
        # Weights for P4_0 and P5_0 to P4_1
        p4_w1 = self.p4_w1_relu(self.p4_w1)
        weight = p4_w1 / (torch.sum(p4_w1, dim=0) + self.epsilon)
        # Connections for P4_0 and P5_0 to P4_1 respectively
        p4_up = self.conv4_up(weight[0] * p4_in + weight[1] * self.p4_upsample(p5_up))

        # Weights for P3_0 and P4_1 to P3_2
        p3_w1 = self.p3_w1_relu(self.p3_w1)
        weight = p3_w1 / (torch.sum(p3_w1, dim=0) + self.epsilon)
        # Connections for P3_0 and P4_1 to P3_2 respectively
        p3_out = self.conv3_up(weight[0] * p3_in + weight[1] * self.p3_upsample(p4_up))

        # Weights for P4_0, P4_1 and P3_2 to P4_2
        p4_w2 = self.p4_w2_relu(self.p4_w2)
        weight = p4_w2 / (torch.sum(p4_w2, dim=0) + self.epsilon)
        # Connections for P4_0, P4_1 and P3_2 to P4_2 respectively
        p4_out = self.conv4_down(
            weight[0] * p4_in + weight[1] * p4_up + weight[2] * self.p4_downsample(p3_out))
        # Weights for P5_0, P5_1 and P4_2 to P5_2
        p5_w2 = self.p5_w2_relu(self.p5_w2)
        weight = p5_w2 / (torch.sum(p5_w2, dim=0) + self.epsilon)
        # Connections for P5_0, P5_1 and P4_2 to P5_2 respectively
        p5_out = self.conv5_down(
            weight[0] * p5_in + weight[1] * p5_up + weight[2] * self.p5_downsample(p4_out))
        # Weights for P6_0, P6_1 and P5_2 to P6_2
        p6_w2 = self.p6_w2_relu(self.p6_w2)
        weight = p6_w2 / (torch.sum(p6_w2, dim=0) + self.epsilon)
        # Connections for P6_0, P6_1 and P5_2 to P6_2 respectively
        p6_out = self.conv6_down(
            weight[0] * p6_in + weight[1] * p6_up + weight[2] * self.p6_downsample(p5_out))
        # Weights for P7_0 and P6_2 to P7_2
        p7_w2 = self.p7_w2_relu(self.p7_w2)
        weight = p7_w2 / (torch.sum(p7_w2, dim=0) + self.epsilon)
        # Connections for P7_0 and P6_2 to P7_2
        p7_out = self.conv7_down(weight[0] * p7_in + weight[1] * self.p7_downsample(p6_out))

        return p3_out, p4_out, p5_out, p6_out, p7_out


class Regressor(nn.Module):
    def __init__(self, in_channels, num_anchors, num_layers):
        super(Regressor, self).__init__()
        layers = []
        for _ in range(num_layers):
            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))
            layers.append(nn.ReLU(True))
        self.layers = nn.Sequential(*layers)
        self.header = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)

    def forward(self, inputs):
        inputs = self.layers(inputs)
        inputs = self.header(inputs)
        output = inputs.permute(0, 2, 3, 1)
        return output.contiguous().view(output.shape[0], -1, 4)


class Classifier(nn.Module):
    def __init__(self, in_channels, num_anchors, num_classes, num_layers):
        super(Classifier, self).__init__()
        self.num_anchors = num_anchors
        self.num_classes = num_classes
        layers = []
        for _ in range(num_layers):
            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))
            layers.append(nn.ReLU(True))
        self.layers = nn.Sequential(*layers)
        self.header = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)
        self.act = nn.Sigmoid()

    def forward(self, inputs):
        inputs = self.layers(inputs)
        inputs = self.header(inputs)
        inputs = self.act(inputs)
        inputs = inputs.permute(0, 2, 3, 1)
        output = inputs.contiguous().view(inputs.shape[0], inputs.shape[1], inputs.shape[2], self.num_anchors,
                                          self.num_classes)
        return output.contiguous().view(output.shape[0], -1, self.num_classes)


class EfficientNet(nn.Module):
    def __init__(self, model_name):
        super(EfficientNet, self).__init__()
        model = EffNet.from_pretrained(model_name)
        del model._conv_head
        del model._bn1
        del model._avg_pooling
        del model._dropout
        del model._fc
        self.model = model

    def forward(self, x):
        x = self.model._swish(self.model._bn0(self.model._conv_stem(x)))
        feature_maps = []
        for idx, block in enumerate(self.model._blocks):
            drop_connect_rate = self.model._global_params.drop_connect_rate
            if drop_connect_rate:
                drop_connect_rate *= float(idx) / len(self.model._blocks)
            x = block(x, drop_connect_rate=drop_connect_rate)
            if block._depthwise_conv.stride == [2, 2]:
                feature_maps.append(x)

        return feature_maps[1:]

class EfficientDet(nn.Module):
    def __init__(self, num_anchors=9, num_classes=20, compound_coef=0, model_name="efficientnet-b0"):
        super(EfficientDet, self).__init__()
        self.compound_coef = compound_coef

        self.num_channels = [64, 88, 112, 160, 224, 288, 384, 384][self.compound_coef]
        ''' why we start from level 3 conv3 to conv7 (beased on Paper "These fused features are fed to a class and box
          network to produce object class and bounding box predictions
          respectively.")'''
        if(self.compound_coef == 0 or self.compound_coef==1):
          self.conv3 = nn.Conv2d(40, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(80, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(192, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(192, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        elif(self.compound_coef == 2):
          self.conv3 = nn.Conv2d(48, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(88, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(208, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(208, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        elif(self.compound_coef == 3):
          self.conv3 = nn.Conv2d(48, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(96, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(232, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(232, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        elif(self.compound_coef == 4):
          self.conv3 = nn.Conv2d(56, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(112, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(272, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(272, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        elif(self.compound_coef == 5):
          self.conv3 = nn.Conv2d(64, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(128, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(304, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(304, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        elif(self.compound_coef == 6):
          self.conv3 = nn.Conv2d(72, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(144, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(344, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(344, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        elif(self.compound_coef == 7):
          self.conv3 = nn.Conv2d(80, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv4 = nn.Conv2d(160, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv5 = nn.Conv2d(384, self.num_channels, kernel_size=1, stride=1, padding=0)
          self.conv6 = nn.Conv2d(384, self.num_channels, kernel_size=3, stride=2, padding=1)
          self.conv7 = nn.Sequential(nn.ReLU(),
                                    nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))
        



        self.bifpn = nn.Sequential(*[BiFPN(self.num_channels) for _ in range(min(2 + self.compound_coef, 8))])

        ''' To understand nn.Sequential
        model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )
         output of model will be

         Sequential(
        (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
        (1): ReLU()
        (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))
        (3): ReLU()
      )

        '''

        self.num_classes = num_classes
        self.regressor = Regressor(in_channels=self.num_channels, num_anchors=num_anchors,
                                   num_layers=3 + self.compound_coef // 3)
        self.classifier = Classifier(in_channels=self.num_channels, num_anchors=num_anchors, num_classes=num_classes,
                                     num_layers=3 + self.compound_coef // 3)

        self.anchors = Anchors()
        self.regressBoxes = BBoxTransform()
        self.clipBoxes = ClipBoxes()
        self.focalLoss = FocalLoss()

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

        prior = 0.01

        self.classifier.header.weight.data.fill_(0)
        self.classifier.header.bias.data.fill_(-math.log((1.0 - prior) / prior))

        self.regressor.header.weight.data.fill_(0)
        self.regressor.header.bias.data.fill_(0)

        self.backbone_net = EfficientNet(model_name)

    def freeze_bn(self):
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.eval()

    def forward(self, inputs):
        if len(inputs) == 2:
            is_training = True
            img_batch, annotations = inputs
        else:
            is_training = False
            img_batch = inputs

        c3, c4, c5 = self.backbone_net(img_batch)
        p3 = self.conv3(c3)
        p4 = self.conv4(c4)
        p5 = self.conv5(c5)
        p6 = self.conv6(c5)
        p7 = self.conv7(p6)

        features = [p3, p4, p5, p6, p7]
        features = self.bifpn(features)

        regression = torch.cat([self.regressor(feature) for feature in features], dim=1)
        classification = torch.cat([self.classifier(feature) for feature in features], dim=1)
        anchors = self.anchors(img_batch)

        if is_training:
            return self.focalLoss(classification, regression, anchors, annotations)
        else:
            transformed_anchors = self.regressBoxes(anchors, regression)
            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)

            scores = torch.max(classification, dim=2, keepdim=True)[0]

            scores_over_thresh = (scores > 0.05)[0, :, 0]

            if scores_over_thresh.sum() == 0:
                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]

            classification = classification[:, scores_over_thresh, :]
            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]
            scores = scores[:, scores_over_thresh, :]

            anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.5)

            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)

            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]

import os
import torch
import numpy as np
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
import cv2
class CocoDataset(Dataset):
    def __init__(self, root_dir, img_dir="images", set_dir='train2017', transform=None):

        self.root_dir = root_dir
        self.img_dir = img_dir
        self.set_name = set_dir
        self.transform = transform
        # read coco json file (Annotation) for either train or valid
        self.coco = COCO(os.path.join(self.root_dir, 'annotations', 'instances_' + self.set_name + '.json'))
        self.image_ids = self.coco.getImgIds()

        self.load_classes()

    def load_classes(self):

        # load class names (name -> label)
        categories = self.coco.loadCats(self.coco.getCatIds())
        categories.sort(key=lambda x: x['id'])

        self.classes = {}
        self.coco_labels = {}
        self.coco_labels_inverse = {}
        for c in categories:
            self.coco_labels[len(self.classes)] = c['id']
            self.coco_labels_inverse[c['id']] = len(self.classes)
            self.classes[c['name']] = len(self.classes)

        # also load the reverse (label -> name)
        self.labels = {}
        for key, value in self.classes.items():
            self.labels[value] = key

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):

        img = self.load_image(idx)
        annot = self.load_annotations(idx)
        sample = {'img': img, 'annot': annot}
        if self.transform:
            sample = self.transform(sample)
        return sample

    def load_image(self, image_index):
        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]
        path = os.path.join(self.root_dir, self.img_dir, self.set_name, image_info['file_name'])
        img = cv2.imread(path)
        # imgae are in BGR color by used cv2 and to get high performance in model we convert it to RGB
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # if len(img.shape) == 2:
        #     img = skimage.color.gray2rgb(img)

        return img.astype(np.float32) / 255.

    def load_annotations(self, image_index):
        # get ground truth annotations
        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)
        annotations = np.zeros((0, 5))

        # some images appear to miss annotations
        if len(annotations_ids) == 0:
            return annotations

        # parse annotations
        coco_annotations = self.coco.loadAnns(annotations_ids)
        for idx, a in enumerate(coco_annotations):

            # some annotations have basically no width / height, skip them
            if a['bbox'][2] < 1 or a['bbox'][3] < 1:
                continue

            annotation = np.zeros((1, 5))
            annotation[0, :4] = a['bbox']
            annotation[0, 4] = self.coco_label_to_label(a['category_id'])
            annotations = np.append(annotations, annotation, axis=0)

        # transform from [x, y, w, h] to [x1, y1, x2, y2]
        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]
        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]

        return annotations

    def coco_label_to_label(self, coco_label):
        return self.coco_labels_inverse[coco_label]

    def label_to_coco_label(self, label):
        return self.coco_labels[label]

    def num_classes(self):
        return len(self.classes)


def collater(data):
    imgs = [s['img'] for s in data]
    annots = [s['annot'] for s in data]
    scales = [s['scale'] for s in data]

    imgs = torch.from_numpy(np.stack(imgs, axis=0))

    max_num_annots = max(annot.shape[0] for annot in annots)

    if max_num_annots > 0:

        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1

        if max_num_annots > 0:
            for idx, annot in enumerate(annots):
                if annot.shape[0] > 0:
                    annot_padded[idx, :annot.shape[0], :] = annot
    else:
        annot_padded = torch.ones((len(annots), 1, 5)) * -1

    imgs = imgs.permute(0, 3, 1, 2)

    return {'img': imgs, 'annot': annot_padded, 'scale': scales}


class Resizer(object):
    """Convert ndarrays in sample to Tensors."""

    def __init__(self, common_size=1024):
        self.common_size = common_size;

    def __call__(self, sample, common_size=1024):
        common_size = self.common_size;
        image, annots = sample['img'], sample['annot']
        height, width, _ = image.shape
        if height > width:
            scale = common_size / height
            resized_height = common_size
            resized_width = int(width * scale)
        else:
            scale = common_size / width
            resized_height = int(height * scale)
            resized_width = common_size

        image = cv2.resize(image, (resized_width, resized_height))
        #print("W * H :", resized_width, " * ", resized_height)
        new_image = np.zeros((common_size, common_size, 3))
        new_image[0:resized_height, 0:resized_width] = image

        annots[:, :4] *= scale

        return {'img': torch.from_numpy(new_image), 'annot': torch.from_numpy(annots), 'scale': scale}

class Augmenter_grayscale(object):
    def __call__(self, sample,gs=0.5):



      """ imgae grayscale """

      if np.random.rand() < gs: # random aug image
        image, annots = sample['img'], sample['annot']
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        stacked_img = np.stack((gray,)*3, axis=-1) 
        sample = {'img': stacked_img, 'annot': annots}    
        
      return sample

class Augmenter_hue(object):
    def __call__(self, sample,  hgain=0.5, sgain=0.0, vgain=0.0,hsv=0.5):



      """ HSV color-space augmentation
      Hue represents the color in range [0.179]  with gain +/- 50 degree
      
      """

      if np.random.rand() < hsv:
        image, annots = sample['img'], sample['annot']
        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
        dtype = image.dtype
        # print(dtype)
        # print(type(dtype))
        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.uint8))

        x = np.arange(0, 256, dtype=dtype)
        lut_hue = ((x * r[0]) % 180)
        lut_sat = np.clip(x * r[1], 0, 255)
        lut_val = np.clip(x * r[2], 0, 255)
        hue = cv2.LUT(hue, lut_hue)
        sat = cv2.LUT(sat, lut_sat)
        val = cv2.LUT(val, lut_val)        
        
        image = cv2.merge((hue, sat, val)).astype(dtype)
        image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR) 
        sample = {'img': image, 'annot': annots}    
      return sample
class Augmenter_saturation(object):
    def __call__(self, sample,  hgain=0.0, sgain=0.5, vgain=0.0,hsv=0.5):



      """ HSV color-space augmentation
      Saturation represents the greyness in range[ 0,255]  with gain +/- 50%
      """

      if np.random.rand() < hsv:
        image, annots = sample['img'], sample['annot']
        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
        dtype = image.dtype
        # print(dtype)
        # print(type(dtype))
        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.uint8))

        x = np.arange(0, 256, dtype=dtype)
        lut_hue = ((x * r[0]) % 180)
        lut_sat = np.clip(x * r[1], 0, 255)
        lut_val = np.clip(x * r[2], 0, 255)
        hue = cv2.LUT(hue, lut_hue)
        sat = cv2.LUT(sat, lut_sat)
        val = cv2.LUT(val, lut_val)        
        
        image = cv2.merge((hue, sat, val)).astype(dtype)
        image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR) 
        sample = {'img': image, 'annot': annots}    
      return sample
class Augmenter_value(object):
    def __call__(self, sample,  hgain=0.0, sgain=0.0, vgain=0.5,hsv=0.5):



      """ HSV color-space augmentation
      Value represents the brightness in range [0,255] with gain +/- 50%
      """
      if np.random.rand() < hsv:
        image, annots = sample['img'], sample['annot']
        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
        dtype = image.dtype
        # print(dtype)
        # print(type(dtype))
        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.uint8))

        x = np.arange(0, 256, dtype=dtype)
        lut_hue = ((x * r[0]) % 180)
        lut_sat = np.clip(x * r[1], 0, 255)
        lut_val = np.clip(x * r[2], 0, 255)
        hue = cv2.LUT(hue, lut_hue)
        sat = cv2.LUT(sat, lut_sat)
        val = cv2.LUT(val, lut_val)        
        
        image = cv2.merge((hue, sat, val)).astype(dtype)
        image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR) 
        sample = {'img': image, 'annot': annots}    
      return sample
class Augmenter_sv(object):
    def __call__(self, sample,  hgain=0.0, sgain=0.5, vgain=0.5,hsv=0.5):



      """ HSV color-space augmentation
       image merge stauration (image grayness with gain +/- 50% ) and value ( image brightness with gain +/- 50%)
      """

      if np.random.rand() < hsv:
        image, annots = sample['img'], sample['annot']
        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
        dtype = image.dtype
        # print(dtype)
        # print(type(dtype))
        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.uint8))

        x = np.arange(0, 256, dtype=dtype)
        lut_hue = ((x * r[0]) % 180)
        lut_sat = np.clip(x * r[1], 0, 255)
        lut_val = np.clip(x * r[2], 0, 255)
        hue = cv2.LUT(hue, lut_hue)
        sat = cv2.LUT(sat, lut_sat)
        val = cv2.LUT(val, lut_val)        
        
        image = cv2.merge((hue, sat, val)).astype(dtype)
        image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR) 
        sample = {'img': image, 'annot': annots}    
      return sample

class Augment_hsv(object):
    def __call__(self, sample,  hgain=0.5, sgain=0.5, vgain=0.5,hsv=0.5):



      """ HSV color-space augmentation
      image merge stauration (image grayness with gain +/- 50% ) and value ( image brightness with gain +/- 50%) and Hue color with gain +/- 50 degree
      """
      if np.random.rand() < hsv:
        image, annots = sample['img'], sample['annot']
        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
        dtype = image.dtype
        # print(dtype)
        # print(type(dtype))
        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.uint8))

        x = np.arange(0, 256, dtype=dtype)
        lut_hue = ((x * r[0]) % 180)
        lut_sat = np.clip(x * r[1], 0, 255)
        lut_val = np.clip(x * r[2], 0, 255)
        hue = cv2.LUT(hue, lut_hue)
        sat = cv2.LUT(sat, lut_sat)
        val = cv2.LUT(val, lut_val)        
        
        image = cv2.merge((hue, sat, val)).astype(dtype)
        image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR) 
        sample = {'img': image, 'annot': annots} 

      return sample

class Augmenter_s_or_v(object):
    def __call__(self, sample,  hgain=0.0, sgain=0.0, vgain=0.0,hsv=0.5):



      """ HSV color-space augmentation
      image aug stauration (image grayness with gain +/- 50% ) Or value (image brightness with gain +/- 50%)
      
      """
      if np.random.rand() < hsv:
        if np.random.rand() < hsv:
          sgain=0.5
        else:
          vgain=0.5
        image, annots = sample['img'], sample['annot']
        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains
        dtype = image.dtype
        # print(dtype)
        # print(type(dtype))
        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.uint8))

        x = np.arange(0, 256, dtype=dtype)
        lut_hue = ((x * r[0]) % 180)
        lut_sat = np.clip(x * r[1], 0, 255)
        lut_val = np.clip(x * r[2], 0, 255)
        hue = cv2.LUT(hue, lut_hue)
        sat = cv2.LUT(sat, lut_sat)
        val = cv2.LUT(val, lut_val)        
        
        image = cv2.merge((hue, sat, val)).astype(dtype)
        image= cv2.cvtColor(image, cv2.COLOR_HSV2BGR) 
        sample = {'img': image, 'annot': annots}    
      return sample
class Augmenter_flip_h(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample, flip_x=0.5):
        if np.random.rand() < flip_x:
            image, annots = sample['img'], sample['annot']
            image = image[:, ::-1, :]#slice [:, ::-1]. The array flips along the second axis.

            rows, cols, channels = image.shape

            x1 = annots[:, 0].copy()
            x2 = annots[:, 2].copy()

            x_tmp = x1.copy()

            annots[:, 0] = cols - x2
            annots[:, 2] = cols - x_tmp

            sample = {'img': image, 'annot': annots}
        return sample

class Augmenter_flip_v(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample, flip_x=0.5):
        if np.random.rand() < flip_x:
            image, annots = sample['img'], sample['annot']
            image = image[ ::-1, :]# slice [::-1]. The array flips along the first axis.

            rows, cols, channels = image.shape

            x1 = annots[:, 1].copy()
            x2 = annots[:, 3].copy()

            x_tmp = x1.copy()

            annots[:, 1] = rows - x2
            annots[:, 3] = rows - x_tmp

            sample = {'img': image, 'annot': annots}
        return sample
  
class Augmenter_RandomFlip(object):

    def __call__(self, sample, do_horizontal=False, do_vertical=False, prob=0.5):
        img,annots = sample['img'], sample['annot']
        rows, cols,_ = img.shape
        if np.random.rand() < prob:
          if np.random.rand() < prob:
            do_horizontal=True
          else:
            do_vertical=True

        if do_horizontal:
          img = img[:, ::-1, :]
          x1 = annots[:, 0].copy()
          x2 = annots[:, 2].copy()
          x_tmp = x1.copy()
          annots[:, 0] = cols - x2
          annots[:, 2] = cols - x_tmp

        elif do_vertical:
          img = img[ ::-1, :]
          x1 = annots[:, 1].copy()
          x2 = annots[:, 3].copy()
          x_tmp = x1.copy()
          annots[:, 1] = rows - x2
          annots[:, 3] = rows - x_tmp

        sample = {'img': img, 'annot': annots}
        return sample
class Augmenter_FlipHV(object):

    def __call__(self, sample, do_horizontal=True, do_vertical=True, prob=0.5):
        img,annots = sample['img'], sample['annot']
        rows, cols,_ = img.shape
        if np.random.rand() < prob:
            img = img[::-1, ::-1, :]#h[:, ::-1, :]v[ ::-1, :]hv [::-1, ::-1][::-1, ::-1, :]
            img = img[:, ::-1, :]
            x1 = annots[:, 0].copy()
            x2 = annots[:, 2].copy()
            x_tmp = x1.copy()
            annots[:, 0] = cols - x2
            annots[:, 2] = cols - x_tmp
            y1 = annots[:, 1].copy()
            y2 = annots[:, 3].copy()
            y_tmp = y1.copy()
            annots[:, 1] = rows - y2
            annots[:, 3] = rows - y_tmp
            sample = {'img': img, 'annot': annots}
        return sample
class Normalizer(object):

    def __init__(self):
        self.mean = np.array([[[0.485, 0.456, 0.406]]])
        self.std = np.array([[[0.229, 0.224, 0.225]]])

    def __call__(self, sample):
        image, annots = sample['img'], sample['annot']

        return {'img': ((image.astype(np.float32) - self.mean) / self.std), 'annot': annots}

"""# Start training"""

# create instance of efficientDet model
gtf = Detector();

#directs the model towards file structure
# chess path "/content/drive/MyDrive/Dataset/Chess_Datasets"
# Full-UAV path "/content/drive/MyDrive/Dataset/Full_UAV"
# MAV_VID path "/content/drive/MyDrive/Dataset/MAV_VID"
# Combine_Drone path "/content/drive/MyDrive/Dataset/Combine_Drone"
root_dir = "";
coco_dir = "/content/drive/MyDrive/Dataset/Combine_Drone";
img_dir = "";
set_dir = "train";

#smells like some free compute from Colab, nice 
# set train set
gtf.Train_Dataset(root_dir, coco_dir, img_dir, set_dir, batch_size=8, image_size=1024, use_gpu=True)

#directs the model towards file structure as as validation
root_dir = "";
coco_dir = "content/drive/MyDrive/Dataset/Combine_Drone";
img_dir = "";
set_dir = "valid";

#smells like some free compute from Colab, nice
# set valid set
gtf.Val_Dataset(root_dir, coco_dir, img_dir, set_dir)

# Available models
# And Set img_size as per model type
# model_name="efficientnet-b0"  image_size=512
# model_name="efficientnet-b1"  image_size=640
# model_name="efficientnet-b2"  image_size=786
# model_name="efficientnet-b3"  image_size=896
# model_name="efficientnet-b4"  image_size=1024
# model_name="efficientnet-b5"  image_size=1280
# model_name="efficientnet-b6"  image_size=1260
# model_name="efficientnet-b7"  image_size=1536
# model_name="efficientnet-b8"  = efficientdet-d7x
# also change ["common_size"] to image size
# only for b0 as default use below line code
gtf.Model(model_name="efficientnet-b0");
# To resume training
#gtf.Model(model_name="efficientnet-b0", load_pretrained_model_from="path to model.pth");

# set Hyper-parameters
gtf.Set_Hyperparams(lr=0.0001, val_interval=1, es_min_delta=0.0, es_patience=0)

""" We train for 100 epochs"""

# Commented out IPython magic to ensure Python compatibility.
# run cells to save weights and model output 
# %mkdir /content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp9
# %mkdir /content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp9/train_weights
# %mkdir /content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp9/tensor_output

# Commented out IPython magic to ensure Python compatibility.
# # start training
# # given file path directorty 
# %%time
# gtf.Train(num_epochs=100, model_output_dir="/content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp9/train_weights",model_output_tensorboard='/content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp9/tensor_output')

# Commented out IPython magic to ensure Python compatibility.
# display the tensor output for train-valid during training step  
# %load_ext tensorboard
# %tensorboard --logdir '/content/drive/MyDrive/Dataset/Combine_Drone//Result_Exp88/tensor_output/'

"""# Inference

Here just  to predict some valid images one or more images and store images after make bbox on objects
"""

import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
#from src.dataset import CocoDataset, Resizer, Normalizer, Augmenter, collater
#from src.model import EfficientDet
from tensorboardX import SummaryWriter
import shutil
import numpy as np
from tqdm.autonotebook import tqdm
#from src.config import colors
import cv2
#import pdb 
colors =[(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),
          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26)]

class Infer():
    '''
    Class for main inference
    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    '''
    def __init__(self, verbose=1):
        self.system_dict = {};
        self.system_dict["verbose"] = verbose;
        self.system_dict["local"] = {};
        self.system_dict["local"]["common_size"] = 1024;
        self.system_dict["local"]["mean"] = np.array([[[0.485, 0.456, 0.406]]])
        self.system_dict["local"]["std"] = np.array([[[0.229, 0.224, 0.225]]])

    def Model(self, model_dir="trained/"):
        '''
        User function: Selet trained model params
        Args:
            model_dir (str): Relative path to directory containing trained models 
        Returns:
            None
        '''
        self.system_dict["local"]["model"] = torch.load(model_dir + "/signatrix_efficientdet_coco.pth").module
        if torch.cuda.is_available():
            self.system_dict["local"]["model"] = self.system_dict["local"]["model"].cuda();

    def Predict(self, img_path, class_list, vis_threshold = 0.4, output_folder = 'Inference'):
        '''
        User function: Run inference on image and visualize it
        Args:
            img_path (str): Relative path to the image file
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved
        Returns:
            tuple: Contaning label IDs, Scores and bounding box locations of predicted objects. 
        '''
        try:
          if not os.path.exists(output_folder):
            os.makedirs(output_folder)
          # image path for image which we wnat to predict it
          image_filename = os.path.basename(img_path)
          # read that img
          img = cv2.imread(img_path);
          # imgae are in BGR color by used cv2 and to get high performance in model we convert it to RGB
          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB);
          image = img.astype(np.float32) / 255.;
          # do Normlizer
          image = (image.astype(np.float32) - self.system_dict["local"]["mean"]) / self.system_dict["local"]["std"]
          # fine Resoluation (Shape) for image
          height, width, _ = image.shape
          # if image size not as common size to resize image
          if height > width: 
              scale = self.system_dict["local"]["common_size"] / height
              resized_height = self.system_dict["local"]["common_size"]
              resized_width = int(width * scale)
          else:
              scale = self.system_dict["local"]["common_size"] / width
              resized_height = int(height * scale)
              resized_width = self.system_dict["local"]["common_size"]
          # scale image resize to fit with common size
          image = cv2.resize(image, (resized_width, resized_height))
          # create initi image to assign our image with same size 
          #print("W * H :", resized_width, " * ", resized_height)
          new_image = np.zeros((self.system_dict["local"]["common_size"], self.system_dict["local"]["common_size"], 3))
          new_image[0:resized_height, 0:resized_width] = image

          img = torch.from_numpy(new_image)

          with torch.no_grad():
            # send our image to our model to predict 
            # permut image to tensor permut(input,Dimension)>>>tesnor
              scores, labels, boxes = self.system_dict["local"]["model"](img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0))
              # print("scores: ",scores)
              # print("labels: ",labels)
              
              boxes /= scale;
              #print("boxes: ",boxes)
        except Exception as e:
          print(e)


        try:

            isDetect=False
            #print("boxes: ",boxes)
            if boxes.shape[0] > 0:
                isDetect=True
                icount=0
                #predCount=0
                output_image = cv2.imread(img_path)
                #print(img_path, "Predected ")
                for box_id in range(boxes.shape[0]):
                    #pdb.set_trace()
                    pred_prob = float(scores[box_id])
                    #icount+=1
                    #print(" for this image : ", img_path)
                    #print("pred_prob  before check Vis_Threshold:",pred_prob)
                    if pred_prob < vis_threshold:
                        #print("pred_prob < 0.5 (Thresold): ", pred_prob)
                        
                        break
                    #print("pred_prob greatThan vis_Threshold:",pred_prob)
                    #predCount+=1
                    icount+=1
                    pred_label = int(labels[box_id])
                    #print("pred_label :",pred_label)
                    xmin, ymin, xmax, ymax = boxes[box_id, :]
                    color = colors[pred_label]
                    cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)
                    text_size = cv2.getTextSize(class_list[pred_label] + ' : %.2f' % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]
                    #print("text_size : ",text_size)
                    cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)
                    cv2.putText(
                        output_image, class_list[pred_label] + ' : %.2f' % pred_prob,
                        (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,
                        (255, 255, 255), 1)

                cv2.imwrite(os.path.join(output_folder, image_filename), output_image)
                cv2.imwrite("output.png", output_image)
                # save reslut of predictions in txt files 
                scores = scores.tolist()
                boxes = boxes.tolist()
                labels = labels.tolist()
                imageName=image_filename.split(".jpg",1)[0] # in Drone test image extenstion is JPEG not jpg or JPG
                #print("output_folder :",output_folder)
                #print("image_filename :",image_filename)
                # we save prediction result in voc format in txt files
                with open(output_folder +"/"+ imageName + '.txt', 'w') as out_file:# txt file contains result of inference img result
                  #write detection result
                  for i in range(len(scores)):
                    #print(scores)
                    line = class_list[labels[i]] + ' ' + str(scores[i]) + ' ' + str(' '.join([str(j) for j in boxes[i]]))
                    out_file.write(line)
                    if i < len(scores) - 1:
                      out_file.write('\n')
              
            return scores, labels, boxes,isDetect,icount
          #else:
              # if not os.path.exists(non_detectedimg):
              #   os.makedirs(non_detectedimg)
              #   output_image_Non_Det = cv2.imread(img_path)
                print("This Image: ",image_filename," Does not detected")
                # cv2.imwrite(os.path.join(non_detectedimg, image_filename), output_image_Non_Det)
              # else:
                
                  # output_image_Non_Det = cv2.imread(img_path)
                  # print("This Image: ",image_filename," Does not detected")
                  # cv2.imwrite(os.path.join(non_detectedimg, image_filename), output_image_Non_Det)
        except Exception as e:
            print("NO Object Detected in image: ",image_filename)
            print("error : ",e)
            # print("boxes.shape[0]: ",boxes.shape[0])
            # print("scores: ",scores)
            # print("labels: ",labels)
            return None
            #return None

    def predict_batch_of_images(self, img_folder, class_list, vis_threshold = 0.4, output_folder='Inference'):
        '''
        User function: Run inference on multiple images and visualize them
        Args:
            img_folder (str): Relative path to folder containing all the image files
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved
        Returns:
            None 
        '''
        all_filenames = os.listdir(img_folder)
        all_filenames.sort()
        generated_count = 0
        for filename in all_filenames:
            img_path = "{}/{}".format(img_folder, filename)
            try:
                self.Predict(img_path , class_list, vis_threshold ,output_folder)
                generated_count += 1
            except:
                continue
        print("Objects detected  for {} images".format(generated_count))

# create instance of inference
gtf = Infer();

#our trained model weights are in here in onxx format
# Given path for saved weights
gtf.Model(model_dir="/content/drive/MyDrive/Dataset/MAV_VID//Result_Exp10/train_weights")

#extract class list from our annotations such as from COCO JSON Fromat
import json
with open('/content/drive/MyDrive/Dataset/Combine_Drone/annotations/instances_train.json') as json_file:
    data = json.load(json_file)
class_list = []
for category in data['categories']:
  class_list.append(category['name'])

class_list

"""# Export Trained Weights"""

# Commented out IPython magic to ensure Python compatibility.
# Export trained weights to use it in future
# %rm -rf trained_export/
# %mkdir trained_export
# %cp ./trained/signatrix_efficientdet_coco.onnx ./trained_export/signatrix_efficientdet_coco_$(date +%F-%H:%M).onnx
# %cp ./trained/signatrix_efficientdet_coco.pth ./trained_export/signatrix_efficientdet_coco_$(date +%F-%H:%M).pth
# Export weights to (Drive) for future used
# %mv ./trained_export/* /content/drive/MyDrive/Retrained_Weights/Dataset/
# convert weight directory to zip file 
!zip -r /content/wieghtfile.zip /content/trained_export
# Download weights in local device
files.download("/content/wieghtfile.zip")

"""# Reloading Trained Weights after Export

Imagine you have exported your trained model and would like to reaccess it later. This portion of the notebook picks up the trained model and starts at inference

(Hint): To avoid error You have to run cells in Section (Pre-processing training)

"""

#export trained model
# mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# For colab use the command below
# Set up library requirments
! cd Monk_Object_Detection/3_mxrcnn/installation && cat requirements_colab.txt | xargs -n 1 -L 1 pip install
#fixed version of tqdm output for Colab
!pip install --force https://github.com/chengs/tqdm/archive/colab.zip
#IGNORE restart runtime warning, it is indeed installed
#missing a few extra packages that we will need later! 
!pip install efficientnet_pytorch
!pip install tensorboardX
!pip install dicttoxml
!pip install  xmltodict
!pip install  torchvision==0.5
!pip install torch==1.4

import os
import sys
sys.path.append("Monk_Object_Detection/4_efficientdet/lib/");

import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
#from src.dataset import CocoDataset, Resizer, Normalizer, Augmenter, collater
#from src.model import EfficientDet
from tensorboardX import SummaryWriter
import shutil
import numpy as np
from tqdm.autonotebook import tqdm
#from src.config import colors
import cv2
colors = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),
          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26)]


class Infer():
    '''
    Class for main inference
    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    '''
    def __init__(self, verbose=1):
        self.system_dict = {};
        self.system_dict["verbose"] = verbose;
        self.system_dict["local"] = {};
        self.system_dict["local"]["common_size"] = 512;
        self.system_dict["local"]["mean"] = np.array([[[0.485, 0.456, 0.406]]])
        self.system_dict["local"]["std"] = np.array([[[0.229, 0.224, 0.225]]])

    def Model(self, model_dir="trained/"):
        '''
        User function: Selet trained model params
        Args:
            model_dir (str): Relative path to directory containing trained models 
        Returns:
            None
        '''
        self.system_dict["local"]["model"] = torch.load(model_dir + "/signatrix_efficientdet_coco.pth").module
        if torch.cuda.is_available():
            self.system_dict["local"]["model"] = self.system_dict["local"]["model"].cuda();

    def Predict(self, img_path, class_list, vis_threshold = 0.4, output_folder = 'Inference'):
        '''
        User function: Run inference on image and visualize it
        Args:
            img_path (str): Relative path to the image file
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved
        Returns:
            tuple: Contaning label IDs, Scores and bounding box locations of predicted objects. 
        '''
        try:
          if not os.path.exists(output_folder):
            os.makedirs(output_folder)
          # image path for image which we wnat to predict it
          image_filename = os.path.basename(img_path)
          # read that img
          img = cv2.imread(img_path);
          # imgae are in BGR color by used cv2 and to get high performance in model we convert it to RGB
          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB);
          image = img.astype(np.float32) / 255.;
          # do Normlizer
          image = (image.astype(np.float32) - self.system_dict["local"]["mean"]) / self.system_dict["local"]["std"]
          # fine Resoluation (Shape) for image
          height, width, _ = image.shape
          # if image size not as common size to resize image
          if height > width: 
              scale = self.system_dict["local"]["common_size"] / height
              resized_height = self.system_dict["local"]["common_size"]
              resized_width = int(width * scale)
          else:
              scale = self.system_dict["local"]["common_size"] / width
              resized_height = int(height * scale)
              resized_width = self.system_dict["local"]["common_size"]
          # scale image resize to fit with common size
          image = cv2.resize(image, (resized_width, resized_height))
          # create initi image to assign our image with same size 
          new_image = np.zeros((self.system_dict["local"]["common_size"], self.system_dict["local"]["common_size"], 3))
          new_image[0:resized_height, 0:resized_width] = image

          img = torch.from_numpy(new_image)

          with torch.no_grad():
            # send our image to our model to predict 
            # permut image to tensor permut(input,Dimension)>>>tesnor
              scores, labels, boxes = self.system_dict["local"]["model"](img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0))
              # print("scores: ",scores)
              # print("labels: ",labels)
              
              boxes /= scale;
              #print("boxes: ",boxes)
        except Exception as e:
          print("errr: ",e)


        try:

            isDetect=False
            #print("boxes: ",boxes)
            if boxes.shape[0] > 0:
                isDetect=True
                icount=0
                #predCount=0
                output_image = cv2.imread(img_path)
                #print(img_path, "Predected ")
                for box_id in range(boxes.shape[0]):
                    
                    pred_prob = float(scores[box_id])
                    #icount+=1
                   # print(" for this image : ", img_path)
                    #print("pred_prob  before check Vis_Threshold:",pred_prob)
                    if pred_prob < vis_threshold:
                        #print("pred_prob < 0.5 (Thresold): ", pred_prob)
                        
                        break
                    #print("pred_prob greatThan vis_Threshold:",pred_prob)
                    #predCount+=1
                    icount+=1
                    pred_label = int(labels[box_id])
                    #print("pred_label :",pred_label)
                    xmin, ymin, xmax, ymax = boxes[box_id, :]
                    color = colors[pred_label]
                    cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)
                    text_size = cv2.getTextSize(class_list[pred_label] + ' : %.2f' % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]

                    cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)
                    cv2.putText(
                        output_image, class_list[pred_label] + ' : %.2f' % pred_prob,
                        (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,
                        (255, 255, 255), 1)

                cv2.imwrite(os.path.join(output_folder, image_filename), output_image)
                cv2.imwrite("output.png", output_image)
                # save reslut of predictions in txt files 
                scores = scores.tolist()
                boxes = boxes.tolist()
                labels = labels.tolist()
                imageName=image_filename.split(".jpg",1)[0] # in Drone test image extenstion is JPEG not jpg or JPG
                #print("output_folder :",output_folder)
                #print("image_filename :",image_filename)
                # we save prediction result in voc format in txt files
                with open(output_folder +"/"+ imageName + '.txt', 'w') as out_file:# txt file contains result of inference img result
                  #write detection result
                  for i in range(len(scores)):
                    #print(scores)
                    line = class_list[labels[i]] + ' ' + str(scores[i]) + ' ' + str(' '.join([str(j) for j in boxes[i]]))
                    out_file.write(line)
                    if i < len(scores) - 1:
                      out_file.write('\n')
                
                return scores, labels, boxes,isDetect,icount
            else:
              # if not os.path.exists(non_detectedimg):
              #   os.makedirs(non_detectedimg)
              #   output_image_Non_Det = cv2.imread(img_path)
                print("This Image: ",image_filename," Does not detected")
                # cv2.imwrite(os.path.join(non_detectedimg, image_filename), output_image_Non_Det)
              # else:
                  # output_image_Non_Det = cv2.imread(img_path)
                  # print("This Image: ",image_filename," Does not detected")
                  # cv2.imwrite(os.path.join(non_detectedimg, image_filename), output_image_Non_Det)
        except Exception as e:
            print("error : ",e)
            # print("boxes.shape[0]: ",boxes.shape[0])
            # print("NO Object Detected in image: ",image_filename)
            #return None

    def predict_batch_of_images(self, img_folder, class_list, vis_threshold = 0.4, output_folder='Inference'):
        '''
        User function: Run inference on multiple images and visualize them
        Args:
            img_folder (str): Relative path to folder containing all the image files
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved
        Returns:
            None 
        '''
        all_filenames = os.listdir(img_folder)
        all_filenames.sort()
        generated_count = 0
        for filename in all_filenames:
            img_path = "{}/{}".format(img_folder, filename)
            try:
                self.Predict(img_path , class_list, vis_threshold ,output_folder)
                generated_count += 1
            except:
                continue
        print("Objects detected  for {} images".format(generated_count))

# create instance
gtf = Infer();

#our trained model weights are in here in onxx format
gtf.Model(model_dir="/content/drive/MyDrive/Dataset/Full_UAV_AUAVD512/Result/train_weights")

#extract class list from our annotations
#in your application you will probably already have this saved
import json
with open('/content/drive/MyDrive/Dataset/MAV_VID/annotations/instances_train.json') as json_file:
    data = json.load(json_file)
class_list = []
for category in data['categories']:
  class_list.append(category['name'])

class_list

"""# Evaluating MAP Performance

Before calcualte the Average Precision, if your Annotation files in different format, you need to convert Ann to PASCAL VOC, Below code help you to do it.
hint: our drone and chess already in VOC format no need conversion

#### Convert Ann format to VOC
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
import os
import glob
import cv2
# %cd /content/
def convert_yolo_coordinates_to_voc(x_c_n, y_c_n, width_n, height_n, img_width, img_height):
  ## remove normalization given the size of the image
  x_c = float(x_c_n) * img_width
  y_c = float(y_c_n) * img_height
  width = float(width_n) * img_width
  height = float(height_n) * img_height
  ## compute half width and half height
  half_width = width / 2
  half_height = height / 2
  ## compute left, top, right, bottom
  ## in the official VOC challenge the top-left pixel in the image has coordinates (1;1)
  left = int(x_c - half_width) + 1
  top = int(y_c - half_height) + 1
  right = int(x_c + half_width) + 1
  bottom = int(y_c + half_height) + 1
  return left, top, right, bottom
def convert_gt_yolo():
  # make sure that the cwd() in the beginning is the location of the python script (so that every path makes sense)
  #os.chdir(os.path.dirname(os.path.abspath(__file__)))

  # read the class_list.txt to a list
  with open("/content/drive/MyDrive/Dataset/Drone_Datasets/valid_test/class_list.txt") as f:
    obj_list = f.readlines()
  ## remove whitespace characters like `\n` at the end of each line
    obj_list = [x.strip() for x in obj_list]
  ## e.g. first object in the list
  #print(obj_list[0])

  # change directory to the one with the files to be changed
  parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))
  parent_path = os.path.abspath(os.path.join(parent_path, os.pardir))
  GT_PATH = os.path.join(parent_path, 'content/drive/MyDrive/Dataset/Drone_Datasets/test/','YOLO_Darknet/')
  print("GT_PATH : ",GT_PATH)
  os.chdir(GT_PATH)
 # old files (YOLO format) will be moved to a new folder (backup/)
  ## create the backup dir if it doesn't exist already
  if not os.path.exists("backup"):
    os.makedirs("backup")

  # create VOC format files
  txt_list = glob.glob('*.txt')
  img_list=[f.split('.jpg')[0] for f in os.listdir('../Image/')]
  print("count img size: ",len(img_list))
  if len(txt_list) == 0:
    print("Error: no .txt files found in ground-truth")
    sys.exit()
  for tmp_file in txt_list:
    #print(tmp_file)
    # 1. check that there is an image with that name
    ## get name before ".txt"
    image_name = tmp_file.split(".txt",1)[0]
    #print(tmp_file)
    ## check if image exists
    # imgPath="/content/drive/MyDrive/Dataset/Drone_Datasets/test/Image/"

    for fname in img_list:
      #print("Fname: ",fname)
      if fname==image_name:
        ## image found
        #print(fname)
        img = cv2.imread('../Image/' + fname+".jpg")
        #print(img)
        ## get image width and height
        img_height, img_width = img.shape[:2]
        break
      # else:
      #   ## image not found
      #   print("Error: image not found : " + image_name)
      #   print("Error: image not found, corresponding to " + tmp_file)
      #   sys.exit()
    # 2. open txt file lines to a list
    #print(tmp_file)
    with open(tmp_file) as f:
      content = f.readlines()
    ## remove whitespace characters like `\n` at the end of each line
    content = [x.strip() for x in content]
    # 3. move old file (YOLO format) to backup
    os.rename(tmp_file, "backup/" + tmp_file)
    # 4. create new file (VOC format)
    with open(tmp_file, "a") as new_f:
      for line in content:
        ## split a line by spaces.
        ## "c" stands for center and "n" stands for normalized
        #print(line)
        obj_id, x_c_n, y_c_n, width_n, height_n = line.split()
        #print((obj_id))
        obj_name = "drone"#obj_list[int(obj_id)]
        left, top, right, bottom = convert_yolo_coordinates_to_voc(x_c_n, y_c_n, width_n, height_n, img_width, img_height)
        ## add new line to file
        #print(obj_name + " " + str(left) + " " + str(top) + " " + str(right) + " " + str(bottom))
        new_f.write(obj_name + " " + str(left) + " " + str(top) + " " + str(right) + " " + str(bottom) + '\n')
  print("Conversion completed!")

# if your test set in YOLO formt run cell below to convert into VOC format
convert_gt_yolo()

"""### Start Predection
>For each class:

>>First, your neural net detection-results are sorted by decreasing confidence and are assigned to ground-truth objects. We have "a match" when they share the same label and an IoU >= 0.5 (Intersection over Union greater than 50%). This "match" is considered a true positive if that ground-truth object has not been already used (to avoid multiple detections of the same object).

"""

# Commented out IPython magic to ensure Python compatibility.
# Create directory in local or in drive to save the evaluation result
# %cd /content/
# %mkdir /content/drive/MyDrive/Dataset/MAV_VID/Result/
# %mkdir /content/drive/MyDrive/Dataset/MAV_VID/Result/detection-results

# save Inference folder in drive which contains images predcted and thier txt files Bboxes
#make model predictions
count_pred_bbox_less_vis=0
total_image=1
Bboxpre=0
for f in os.listdir('/content/drive/MyDrive/Dataset/MAV_VID/test/'):
  try:
    
    if f.endswith('.jpg'):
      #print(f)
      total_image+=1
      # vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
      scores, labels, boxes, count,countBbox= gtf.Predict('/content/drive/MyDrive/Dataset/MAV_VID/test/'+f, class_list, vis_threshold=0.5,
                                                          output_folder='/content/drive/MyDrive/Dataset/MAV_VID/Result/detection-results/');
      if count:
        #print(f,"has Bbox: ",countBbox)
        Bboxpre+=countBbox
        count_pred_bbox_less_vis+=1

  except Exception as e:
    print(e)
    continue
print("# Toall images are : ",total_image)
print("# Images are not predected are : ",total_image-count_pred_bbox_less_vis )#," but we generated empty txt file for each image to avoid mis intersection between GT and DR ")
print("# Prediction images are : ",count_pred_bbox_less_vis)
print("# Toall Bbox are predected with BBox > 0.5 : ",Bboxpre)

#count detected images
print((len([name for name in os.listdir('/content/drive/MyDrive/Dataset/UAV/Result/detection-results/') if os.path.isfile(os.path.join('/content/drive/MyDrive/Dataset/UAV/Result/detection-results/', name))]))/2)

# Show images Prediction
imagepaths=[]
for i in range(5):
  test_images = [f for f in os.listdir('/content/drive/MyDrive/Dataset/Combine_Drone/Result/detection-results/') if f.endswith('.jpg')]
  import random
  img_path = "/content/drive/MyDrive/Dataset/Combine_Drone/Result/detection-results/" + random.choice(test_images);
  imagepaths.append(img_path)

# display images
from IPython.display import Image
for path in imagepaths:
  #Image(filename=path) 
  img=cv2.imread(path)
  print(img.shape[:2])
  x = Image(filename=path)
  display(x)

"""##### Calculate Average Precison
###Calculate mAP
>We calculate the mean of all the AP's, resulting in an mAP value from 0 to 100%.
"""

import glob
import json
import os
import shutil
import operator
import sys
import argparse
import math
import cv2
import numpy as np
import matplotlib.pyplot as plt

def log_average_miss_rate(prec, rec, num_images):
    """
        log-average miss rate:
            Calculated by averaging miss rates at 9 evenly spaced FPPI points
            between 10e-2 and 10e0, in log-space.

        output:
                lamr | log-average miss rate
                mr | miss rate
                fppi | false positives per image

        references:
            [1] Dollar, Piotr, et al. "Pedestrian Detection: An Evaluation of the
               State of the Art." Pattern Analysis and Machine Intelligence, IEEE
               Transactions on 34.4 (2012): 743 - 761.
    """

    # if there were no detections of that class
    if prec.size == 0:
        lamr = 0
        mr = 1
        fppi = 0
        return lamr, mr, fppi

    fppi = (1 - prec)
    mr = (1 - rec)

    fppi_tmp = np.insert(fppi, 0, -1.0)
    mr_tmp = np.insert(mr, 0, 1.0)

    # Use 9 evenly spaced reference points in log-space
    ref = np.logspace(-2.0, 0.0, num=9)
    for i, ref_i in enumerate(ref):
        # np.where() will always find at least 1 index, since min(ref) = 0.01 and min(fppi_tmp) = -1.0
        j = np.where(fppi_tmp <= ref_i)[-1][-1]
        ref[i] = mr_tmp[j]

    # log(0) is undefined, so we use the np.maximum(1e-10, ref)
    lamr = math.exp(np.mean(np.log(np.maximum(1e-10, ref))))

    return lamr, mr, fppi


"""
 throw error and exit
"""


def error(msg):
    print(msg)
   ## sys.exit(0)


"""
 check if the number is a float between 0.0 and 1.0
"""


def is_float_between_0_and_1(value):
    try:
        val = float(value)
        if val > 0.0 and val < 1.0:
            return True
        else:
            return False
    except ValueError:
        return False


"""
 Calculate the AP given the recall and precision array
    1st) We compute a version of the measured precision/recall curve with
         precision monotonically decreasing
    2nd) We compute the AP as the area under this curve by numerical integration.
"""


def voc_ap(rec, prec):
    """
    --- Official matlab code VOC2012---
    mrec=[0 ; rec ; 1];
    mpre=[0 ; prec ; 0];
    for i=numel(mpre)-1:-1:1
            mpre(i)=max(mpre(i),mpre(i+1));
    end
    i=find(mrec(2:end)~=mrec(1:end-1))+1;
    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));
    """
    rec.insert(0, 0.0)  # insert 0.0 at begining of list
    rec.append(1.0)  # insert 1.0 at end of list
    mrec = rec[:]
    prec.insert(0, 0.0)  # insert 0.0 at begining of list
    prec.append(0.0)  # insert 0.0 at end of list
    mpre = prec[:]
    """
     This part makes the precision monotonically decreasing
        (goes from the end to the beginning)
        matlab: for i=numel(mpre)-1:-1:1
                    mpre(i)=max(mpre(i),mpre(i+1));
    """
    # matlab indexes start in 1 but python in 0, so I have to do:
    #     range(start=(len(mpre) - 2), end=0, step=-1)
    # also the python function range excludes the end, resulting in:
    #     range(start=(len(mpre) - 2), end=-1, step=-1)
    for i in range(len(mpre) - 2, -1, -1):
        mpre[i] = max(mpre[i], mpre[i + 1])
    """
     This part creates a list of indexes where the recall changes
        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;
    """
    i_list = []
    for i in range(1, len(mrec)):
        if mrec[i] != mrec[i - 1]:
            i_list.append(i)  # if it was matlab would be i + 1
    """
     The Average Precision (AP) is the area under the curve
        (numerical integration)
        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));
    """
    ap = 0.0
    for i in i_list:
        ap += ((mrec[i] - mrec[i - 1]) * mpre[i])
    return ap, mrec, mpre


"""
 Convert the lines of a file to a list
"""


def file_lines_to_list(path):
    # open txt file lines to a list
    with open(path) as f:
        content = f.readlines()
    # remove whitespace characters like `\n` at the end of each line
    content = [x.strip() for x in content]
    return content


"""
 Draws text in image
"""


def draw_text_in_image(img, text, pos, color, line_width):
    font = cv2.FONT_HERSHEY_PLAIN
    fontScale = 1
    lineType = 1
    bottomLeftCornerOfText = pos
    cv2.putText(img, text,
                bottomLeftCornerOfText,
                font,
                fontScale,
                color,
                lineType)
    text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]
    return img, (line_width + text_width)


"""
 Plot - adjust axes
"""


def adjust_axes(r, t, fig, axes):
    # get text width for re-scaling
    bb = t.get_window_extent(renderer=r)
    text_width_inches = bb.width / fig.dpi
    # get axis width in inches
    current_fig_width = fig.get_figwidth()
    new_fig_width = current_fig_width + text_width_inches
    propotion = new_fig_width / current_fig_width
    # get axis limit
    x_lim = axes.get_xlim()
    axes.set_xlim([x_lim[0], x_lim[1] * propotion])


"""
 Draw plot using Matplotlib
"""


def draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color,
                   true_p_bar):
    # sort the dictionary by decreasing value, into a list of tuples
    sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))
    # unpacking the list of tuples into two lists
    sorted_keys, sorted_values = zip(*sorted_dic_by_value)
    #
    if true_p_bar != "":
        """
         Special case to draw in:
            - green -> TP: True Positives (object detected and matches ground-truth)
            - red -> FP: False Positives (object detected but does not match ground-truth)
            - pink -> FN: False Negatives (object not detected but present in the ground-truth)
        """
        fp_sorted = []
        tp_sorted = []
        for key in sorted_keys:
            fp_sorted.append(dictionary[key] - true_p_bar[key])
            tp_sorted.append(true_p_bar[key])
        plt.barh(range(n_classes), fp_sorted, align='center', color='crimson', label='False Positive')
        plt.barh(range(n_classes), tp_sorted, align='center', color='forestgreen', label='True Positive',
                 left=fp_sorted)
        # add legend
        plt.legend(loc='lower right')
        """
         Write number on side of bar
        """
        fig = plt.gcf()  # gcf - get current figure
        axes = plt.gca()
        r = fig.canvas.get_renderer()
        for i, val in enumerate(sorted_values):
            fp_val = fp_sorted[i]
            tp_val = tp_sorted[i]
            fp_str_val = " " + str(fp_val)
            tp_str_val = fp_str_val + " " + str(tp_val)
            # trick to paint multicolor with offset:
            # first paint everything and then repaint the first number
            t = plt.text(val, i, tp_str_val, color='forestgreen', va='center', fontweight='bold')
            plt.text(val, i, fp_str_val, color='crimson', va='center', fontweight='bold')
            if i == (len(sorted_values) - 1):  # largest bar
                adjust_axes(r, t, fig, axes)
    else:
        plt.barh(range(n_classes), sorted_values, color=plot_color)
        """
         Write number on side of bar
        """
        fig = plt.gcf()  # gcf - get current figure
        axes = plt.gca()
        r = fig.canvas.get_renderer()
        for i, val in enumerate(sorted_values):
            str_val = " " + str(val)  # add a space before
            if val < 1.0:
                str_val = " {0:.2f}".format(val)
            t = plt.text(val, i, str_val, color=plot_color, va='center', fontweight='bold')
            # re-set axes to show number inside the figure
            if i == (len(sorted_values) - 1):  # largest bar
                adjust_axes(r, t, fig, axes)
    # set window title
#    fig.canvas.set_window_title(window_title)
    # write classes in y axis
    tick_font_size = 12
    plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)
    """
     Re-scale height accordingly
    """
    init_height = fig.get_figheight()
    # comput the matrix height in points and inches
    dpi = fig.dpi
    height_pt = n_classes * (tick_font_size * 1.4)  # 1.4 (some spacing)
    height_in = height_pt / dpi
    # compute the required figure height
    top_margin = 0.15  # in percentage of the figure height
    bottom_margin = 0.05  # in percentage of the figure height
    figure_height = height_in / (1 - top_margin - bottom_margin)
    # set new height
    if figure_height > init_height:
        fig.set_figheight(figure_height)

    # set plot title
    plt.title(plot_title, fontsize=14)
    # set axis titles
    # plt.xlabel('classes')
    plt.xlabel(x_label, fontsize='large')
    # adjust size of window
    fig.tight_layout()
    # save the plot
    fig.savefig(output_path)
    # show image
    if to_show:
        plt.show()
    # close the plot
    plt.close()


def calcualte_mAP():
    MINOVERLAP = 0.5  # default value (defined in the PASCAL VOC2012 challenge)

    '''
        0,0 ------> x (width)
        |
        |  (Left,Top)
        |      *_________
        |      |         |
                |         |
        y      |_________|
    (height)            *
                    (Right,Bottom)
    '''

    GT_PATH =  '/content/drive/MyDrive/Dataset/Combine_Drone/test/Voc_Format/'
    DR_PATH =  '/content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp10/detection-results/'
    #print("GT_PATH : ",GT_PATH)
    #print("DR_PATH : ",DR_PATH)
    # if there are no images then no animation can be shown
    IMG_PATH ='/content/drive/MyDrive/Dataset/Combine_Drone/test/'
    draw_plot = True
    """
    Create a ".temp_files/" and "output/" directory
    """
    TEMP_FILES_PATH = ".temp_files"
    if not os.path.exists(TEMP_FILES_PATH):  # if it doesn't exist already
        os.makedirs(TEMP_FILES_PATH)
    output_files_path = "/content/drive/MyDrive/Dataset/Combine_Drone/Result_Exp10/detection-results/output/"
    if os.path.exists(output_files_path):  # if it exist already
        # reset the output directory
        shutil.rmtree(output_files_path)

    os.makedirs(output_files_path)
    # folder to save result for each class
    os.makedirs(os.path.join(output_files_path, "classes"))
    # folder to save images prediction
    os.makedirs(os.path.join(output_files_path, "images"))
     # folder to save images not prediction   
    os.makedirs(os.path.join(output_files_path, "images/images_not_detected"))
    # folder to save images prediction but not satisfy with our threshold
    os.makedirs(os.path.join(output_files_path, "images/images_not_match_thresold"))
    """
    ground-truth
        Load each of the ground-truth files into a temporary ".json" file.
        Create a list of all the class names present in the ground-truth (gt_classes).
    """
    # get a list with the ground-truth files
    ground_truth_files_list = glob.glob(GT_PATH + '/*.txt')
    if len(ground_truth_files_list) == 0:
        error("Error: No ground-truth files found!")
    ground_truth_files_list.sort()
    # dictionary with counter per class
    gt_counter_per_class = {}
    counter_images_per_class = {}

    gt_files = []
    # dictionary contains files and bbox not match our threshold >0.5
    count_not_match_threshold=dict()
    print("GT_Files: ",len(ground_truth_files_list))
    for txt_file in ground_truth_files_list:
        # print(txt_file)
        file_id = txt_file.split(".txt", 1)[0]
        file_id = os.path.basename(os.path.normpath(file_id))
        # check if there is a correspondent detection-results file
        temp_path = os.path.join(DR_PATH, (file_id + ".txt"))
        if not os.path.exists(temp_path):
            error_msg = "Error. File not found: {}\n".format(temp_path)
            error_msg += "(You can avoid this error message by running extra/intersect-gt-and-dr.py)"
            error(error_msg)
        lines_list = file_lines_to_list(txt_file)
        # create ground-truth dictionary
        bounding_boxes = []
        is_difficult = False
        already_seen_classes = []
        for line in lines_list:
            try:
                if "difficult" in line:
                    class_name, left, top, right, bottom, _difficult = line.split()
                    is_difficult = True
                else:
                    class_name, left, top, right, bottom = line.split()
            except ValueError:
                error_msg = "Error: File " + txt_file + " in the wrong format.\n"
                error_msg += " Expected: <class_name> <left> <top> <right> <bottom> ['difficult']\n"
                error_msg += " Received: " + line
                error_msg += "\n\nIf you have a <class_name> with spaces between words you should remove them\n"
                error_msg += "by running the script \"remove_space.py\" or \"rename_class.py\" in the \"extra/\" folder."
                error(error_msg)
            # check if class is in the ignore list, if yes skip
            # if class_name in args.ignore:
            #     continue
            bbox = left + " " + top + " " + right + " " + bottom
            if is_difficult:
                bounding_boxes.append({"class_name": class_name, "bbox": bbox, "used": False, "difficult": True})
                is_difficult = False
            else:
                bounding_boxes.append({"class_name": class_name, "bbox": bbox, "used": False})
                # count that object
                if class_name in gt_counter_per_class:
                    gt_counter_per_class[class_name] += 1
                else:
                    # if class didn't exist yet
                    gt_counter_per_class[class_name] = 1

                if class_name not in already_seen_classes:
                    if class_name in counter_images_per_class:
                        counter_images_per_class[class_name] += 1
                    else:
                        # if class didn't exist yet
                        counter_images_per_class[class_name] = 1
                    already_seen_classes.append(class_name)

        # dump bounding_boxes into a ".json" file
        new_temp_file = TEMP_FILES_PATH + "/" + file_id + "_ground_truth.json"
        gt_files.append(new_temp_file)
        with open(new_temp_file, 'w') as outfile:
            json.dump(bounding_boxes, outfile)

    gt_classes = list(gt_counter_per_class.keys())
    # let's sort the classes alphabetically
    gt_classes = sorted(gt_classes)
    n_classes = len(gt_classes)
    # print(gt_classes)
    # print(gt_counter_per_class)

    """
    detection-results
        Load each of the detection-results files into a temporary ".json" file.
    """
    # get a list with the detection-results files
    dr_files_list = glob.glob(DR_PATH + '/*.txt')
    dr_files_list.sort()
    print("DR_Files :",len(dr_files_list))
    for class_index, class_name in enumerate(gt_classes):
        bounding_boxes = []
        for txt_file in dr_files_list:
            # print(txt_file)
            # the first time it checks if all the corresponding ground-truth files exist
            file_id = txt_file.split(".txt", 1)[0]
            file_id = os.path.basename(os.path.normpath(file_id))
            temp_path = os.path.join(GT_PATH, (file_id + ".txt"))
            if class_index == 0:
                if not os.path.exists(temp_path):
                    error_msg = "Error. File not found: {}\n".format(temp_path)
                    error_msg += "(You can avoid this error message by running extra/intersect-gt-and-dr.py)"
                    error(error_msg)
            lines = file_lines_to_list(txt_file)
            for line in lines:
                try:
                    tmp_class_name, confidence, left, top, right, bottom = line.split()
                except ValueError:
                    error_msg = "Error: File " + txt_file + " in the wrong format.\n"
                    error_msg += " Expected: <class_name> <confidence> <left> <top> <right> <bottom>\n"
                    error_msg += " Received: " + line
                    error(error_msg)
                if tmp_class_name == class_name:
                    # print("match")
                    bbox = left + " " + top + " " + right + " " + bottom
                    bounding_boxes.append({"confidence": confidence, "file_id": file_id, "bbox": bbox})
                    # print(bounding_boxes)
        # sort detection-results by decreasing confidence
        bounding_boxes.sort(key=lambda x: float(x['confidence']), reverse=True)
        with open(TEMP_FILES_PATH + "/" + class_name + "_dr.json", 'w') as outfile:
            json.dump(bounding_boxes, outfile)

    """
    Calculate the AP for each class
    """
    sum_AP = 0.0
    ap_dictionary = {}
    lamr_dictionary = {}
    # open file to store the output
    with open(output_files_path + "/output.txt", 'w') as output_file:
        output_file.write("# AP and precision/recall per class\n")
        count_true_positives = {}
        for class_index, class_name in enumerate(gt_classes):
            count_true_positives[class_name] = 0
            """
            Load detection-results of that class
            """
            dr_file = TEMP_FILES_PATH + "/" + class_name + "_dr.json"
            dr_data = json.load(open(dr_file))

            """
            Assign detection-results to ground-truth objects
            """
            nd = len(dr_data)
            tp = [0] * nd  # creates an array of zeros of size nd
            fp = [0] * nd

            for idx, detection in enumerate(dr_data):
                file_id = detection["file_id"]

                gt_file = TEMP_FILES_PATH + "/" + file_id + "_ground_truth.json"
                ground_truth_data = json.load(open(gt_file))
                ovmax = -1
                gt_match = -1
                # load detected object bounding-box
                bb = [float(x) for x in detection["bbox"].split()]
                for obj in ground_truth_data:
                    # look for a class_name match
                    if obj["class_name"] == class_name:
                        bbgt = [float(x) for x in obj["bbox"].split()]
                        bi = [max(bb[0], bbgt[0]), max(bb[1], bbgt[1]), min(bb[2], bbgt[2]), min(bb[3], bbgt[3])]
                        iw = bi[2] - bi[0] + 1
                        ih = bi[3] - bi[1] + 1
                        if iw > 0 and ih > 0:
                            # compute overlap (IoU) = area of intersection / area of union
                            ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]
                                                                              + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih
                            ov = iw * ih / ua
                            if ov > ovmax:
                                ovmax = ov
                                gt_match = obj

                # assign detection as true positive/don't care/false positive
                # if show_animation:
                #     status = "NO MATCH FOUND!" # status is only used in the animation
                # set minimum overlap
                min_overlap = MINOVERLAP
                # if specific_iou_flagged:
                #     if class_name in specific_iou_classes:
                #         index = specific_iou_classes.index(class_name)
                #         min_overlap = float(iou_list[index])
                if ovmax >= min_overlap:
                    if "difficult" not in gt_match:
                        if not bool(gt_match["used"]):
                            # true positive
                            # print("tp: ",ovmax)
                            tp[idx] = 1
                            gt_match["used"] = True
                            count_true_positives[class_name] += 1
                            # update the ".json" file
                            with open(gt_file, 'w') as f:
                                f.write(json.dumps(ground_truth_data))
                            # if show_animation:
                            #     status = "MATCH!"
                        else:
                            # false positive (multiple detection)
                            fp[idx] = 1
                            # print("fp: ",ovmax)
                            # if show_animation:
                            #     status = "REPEATED MATCH!"
                else:
                    # false positive
                    count_not_match_threshold[file_id]=bb
                    fp[idx] = 1
                    if ovmax > 0:
                        status = "INSUFFICIENT OVERLAP"

            # print(tp)
            # compute precision/recall
            cumsum = 0
            for idx, val in enumerate(fp):
                fp[idx] += cumsum
                cumsum += val
            cumsum = 0
            for idx, val in enumerate(tp):
                tp[idx] += cumsum
                cumsum += val
            # print(tp)
            rec = tp[:]
            for idx, val in enumerate(tp):
                rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]
            # print(rec)
            prec = tp[:]
            #print(len(tp))
            #print(len(fp))

            for idx, val in enumerate(tp):
                prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])
            #print(len(prec))

            ap, mrec, mprec = voc_ap(rec[:], prec[:])
            #print(len(mprec))
            sum_AP += ap
            text = "{0:.2f}%".format(
                ap * 100) + " = " + class_name + " AP "  # class_name + " AP = {0:.2f}%".format(ap*100)
            """
            Write to output.txt
            """
            rounded_prec = ['%.2f' % elem for elem in prec]
            rounded_rec = ['%.2f' % elem for elem in rec]
            output_file.write(text + "\n Precision: " + str(rounded_prec) + "\n Recall :" + str(rounded_rec) + "\n\n")
            # if not args.quiet:
            #     print(text)
            ap_dictionary[class_name] = ap

            n_images = counter_images_per_class[class_name]
            lamr, mr, fppi = log_average_miss_rate(np.array(prec), np.array(rec), n_images)
            lamr_dictionary[class_name] = lamr

            """
            Draw plot
            """
            if draw_plot:
                plt.plot(rec, prec, '-o')
                # add a new penultimate point to the list (mrec[-2], 0.0)
                # since the last line segment (and respective area) do not affect the AP value
                area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]
                area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]
                plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor='r')
                # set window title
                fig = plt.gcf()  # gcf - get current figure
                #fig.canvas.set_window_title('AP ' + class_name)
                # set plot title
                plt.title('class: ' + text)
                # plt.suptitle('This is a somewhat long figure title', fontsize=16)
                # set axis titles
                plt.xlabel('Recall')
                plt.ylabel('Precision')
                # optional - set axes
                axes = plt.gca()  # gca - get current axes
                axes.set_xlim([0.0, 1.0])
                axes.set_ylim([0.0, 1.05])  # .05 to give some extra space
                # Alternative option -> wait for button to be pressed
                # while not plt.waitforbuttonpress(): pass # wait for key display
                # Alternative option -> normal display
                # plt.show()
                # save the plot
                fig.savefig(output_files_path + "/classes/" + class_name + ".png")
                plt.cla()  # clear axes for next plot

        # if show_animation:
        #     cv2.destroyAllWindows()

        output_file.write("\n# mAP of all classes\n")
        mAP = sum_AP / n_classes
        text = "mAP = {0:.2f}%".format(mAP * 100)
        output_file.write(text + "\n")
        print(text)

    """
    Draw false negatives
    """
    if draw_plot:
        pink = (203,192,255)
        green=(0,255,0)
        red=	(139,0,0)
        for tmp_file in gt_files:
            ground_truth_data = json.load(open(tmp_file))
            #print(ground_truth_data)
            # get name of corresponding image
            start = TEMP_FILES_PATH + '/'
            img_id = tmp_file[tmp_file.find(start)+len(start):tmp_file.rfind('_ground_truth.json')]
            img_cumulative_path = output_files_path + "/images/images_not_match_thresold/" + img_id + ".jpg"
            img = cv2.imread(img_cumulative_path)
            if img is None:
                img_path = IMG_PATH + '/' + img_id + ".jpg"
                img = cv2.imread(img_path)
            # draw false negatives
            for obj in ground_truth_data:
                if not obj['used']:
                    if img_id in count_not_match_threshold.keys():
                      bbgt = [ int(round(float(x))) for x in obj["bbox"].split() ]
                      # draw bbox of ground truth
                      cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),green,2)
                      # draw bbox of detection
                      bbx=count_not_match_threshold[img_id]
                      #print(bbx)
                      cv2.rectangle(img,(int(bbx[0]),int(bbx[1])),(int(bbx[2]),int(bbx[3])),red,2)
                      cv2.imwrite(img_cumulative_path, img)
                    else:
                      # draw bbox for images not detected
                      bbgt = [ int(round(float(x))) for x in obj["bbox"].split() ]
                      cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),pink,2)
                      imgPath=output_files_path + "/images/images_not_detected/" + img_id + ".jpg"
                      cv2.imwrite(imgPath, img)
    # remove the temp_files directory
    shutil.rmtree(TEMP_FILES_PATH)

    """
    Count total of detection-results
    """
    # iterate through all the files
    det_counter_per_class = {}
    for txt_file in dr_files_list:
        # get lines to list
        lines_list = file_lines_to_list(txt_file)
        for line in lines_list:
            class_name = line.split()[0]
            # check if class is in the ignore list, if yes skip
            # if class_name in args.ignore:
            #     continue
            # count that object
            if class_name in det_counter_per_class:
                det_counter_per_class[class_name] += 1
            else:
                # if class didn't exist yet
                det_counter_per_class[class_name] = 1
    # print(det_counter_per_class)
    dr_classes = list(det_counter_per_class.keys())

    """
    Plot the total number of occurences of each class in the ground-truth
    """
    if draw_plot:
        window_title = "ground-truth-info"
        plot_title = "ground-truth\n"
        plot_title += "(" + str(len(ground_truth_files_list)) + " files and " + str(n_classes) + " classes)"
        x_label = "Number of objects per class"
        output_path = output_files_path + "/ground-truth-info.png"
        to_show = False
        plot_color = 'forestgreen'
        draw_plot_func(
            gt_counter_per_class,
            n_classes,
            window_title,
            plot_title,
            x_label,
            output_path,
            to_show,
            plot_color,
            '',
        )

    """
    Write number of ground-truth objects per class to results.txt
    """
    with open(output_files_path + "/output.txt", 'a') as output_file:
        output_file.write("\n# Number of ground-truth objects per class\n")
        for class_name in sorted(gt_counter_per_class):
            output_file.write(class_name + ": " + str(gt_counter_per_class[class_name]) + "\n")

    """
    Finish counting true positives
    """
    for class_name in dr_classes:
        # if class exists in detection-result but not in ground-truth then there are no true positives in that class
        if class_name not in gt_classes:
            count_true_positives[class_name] = 0
    # print(count_true_positives)

    """
    Plot the total number of occurences of each class in the "detection-results" folder
    """
    if draw_plot:
        window_title = "detection-results-info"
        # Plot title
        plot_title = "detection-results\n"
        plot_title += "(" + str(len(dr_files_list)) + " files and "
        count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(det_counter_per_class.values()))
        plot_title += str(count_non_zero_values_in_dictionary) + " detected classes)"
        # end Plot title
        x_label = "Number of objects per class"
        output_path = output_files_path + "/detection-results-info.png"
        to_show = False
        plot_color = 'forestgreen'
        true_p_bar = count_true_positives
        draw_plot_func(
            det_counter_per_class,
            len(det_counter_per_class),
            window_title,
            plot_title,
            x_label,
            output_path,
            to_show,
            plot_color,
            true_p_bar
        )

    """
    Write number of detected objects per class to output.txt
    """
    with open(output_files_path + "/output.txt", 'a') as output_file:
        output_file.write("\n# Number of detected objects per class\n")
        for class_name in sorted(dr_classes):
            n_det = det_counter_per_class[class_name]
            text = class_name + ": " + str(n_det)
            text += " (tp:" + str(count_true_positives[class_name]) + ""
            text += ", fp:" + str(n_det - count_true_positives[class_name]) + ")\n"
            output_file.write(text)

    """
    Draw log-average miss rate plot (Show lamr of all classes in decreasing order)
    """
    if draw_plot:
        window_title = "lamr"
        plot_title = "log-average miss rate"
        x_label = "log-average miss rate"
        output_path = output_files_path + "/lamr.png"
        to_show = False
        plot_color = 'royalblue'
        draw_plot_func(
            lamr_dictionary,
            n_classes,
            window_title,
            plot_title,
            x_label,
            output_path,
            to_show,
            plot_color,
            ""
        )

    """
    Draw mAP plot (Show AP's of all classes in decreasing order)
    """
    if draw_plot:
        window_title = "mAP"
        plot_title = "mAP = {0:.2f}%".format(mAP * 100)
        x_label = "Average Precision"
        output_path = output_files_path + "/mAP.png"
        to_show = True
        plot_color = 'royalblue'
        draw_plot_func(
            ap_dictionary,
            n_classes,
            window_title,
            plot_title,
            x_label,
            output_path,
            to_show,
            plot_color,
            ""
        )

# run mAP calculate
calcualte_mAP()

"""###Intersect ground-truth and detection-results files
> This script ensures same number of files in ground-truth and detection-results folder. When you encounter file not found error, it's usually because you have mismatched numbers of ground-truth and detection-results files. You can use this script to move ground-truth and detection-results files that are not in the intersection into a backup folder (backup_no_matches_found). This will retain only files that have the same name in both folders.
"""

import sys
import os
import glob

## This script ensures same number of files in ground-truth and detection-results folder.
## When you encounter file not found error, it's usually because you have
## mismatched numbers of ground-truth and detection-results files.
## You can use this script to move ground-truth and detection-results files that are
## not in the intersection into a backup folder (backup_no_matches_found).
## This will retain only files that have the same name in both folders.

def intersect_gt_and_dr():
  
  # make sure that the cwd() in the beginning is the location of the python script (so that every path makes sense)
  #os.chdir(os.path.dirname(os.path.abspath(__file__)))

  parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))
  parent_path = os.path.abspath(os.path.join(parent_path, os.pardir))
  GT_PATH =  '/content/drive/MyDrive/Dataset/Combine_Drone//test/Voc_Format/'
  DR_PATH =  '/content/drive/MyDrive/Dataset/Combine_Drone/Result/detection-results/'

  backup_folder = 'backup_no_matches_found' # must end without slash

  os.chdir(GT_PATH)
  gt_files = glob.glob('*.txt')
  if len(gt_files) == 0:
      print("Error: no .txt files found in", GT_PATH)
      sys.exit()
  os.chdir(DR_PATH)
  dr_files = glob.glob('*.txt')
  if len(dr_files) == 0:
      print("Error: no .txt files found in", DR_PATH)
      sys.exit()

  gt_files = set(gt_files)
  dr_files = set(dr_files)
  print('total ground-truth files:', len(gt_files))
  print('total detection-results files:', len(dr_files))
  print()

  gt_backup = gt_files - dr_files
  dr_backup = dr_files - gt_files

  def backup(src_folder, backup_files, backup_folder):
      # non-intersection files (txt format) will be moved to a backup folder
      if not backup_files:
          print('No backup required for', src_folder)
          return
      os.chdir(src_folder)
      ## create the backup dir if it doesn't exist already
      if not os.path.exists(backup_folder):
          os.makedirs(backup_folder)
      for file in backup_files:
          os.rename(file, backup_folder + '/' + file)
      
  backup(GT_PATH, gt_backup, backup_folder)
  backup(DR_PATH, dr_backup, backup_folder)
  if gt_backup:
      print('total ground-truth backup files:', len(gt_backup))
  if dr_backup:
      print('total detection-results backup files:', len(dr_backup))

  intersection = gt_files & dr_files
  print('total intersected files:', len(intersection))
  print("Intersection completed!")

intersect_gt_and_dr()